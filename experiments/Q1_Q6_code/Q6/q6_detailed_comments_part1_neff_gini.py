#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ============================================================================
# Q6 负载集中度分析脚本 - 详细注释补充（第1部分）
# n_eff（有效专家数）和 Gini 系数
# ============================================================================

"""
========================================================================
本部分包含 Q6 的核心函数：
1. n_eff_entropy：熵口径的有效专家数
2. n_eff_simpson：Simpson 口径的有效专家数
3. gini_coefficient：Gini 系数
========================================================================
"""

# ============================================================================
# 核心函数：n_eff_entropy（熵口径的有效专家数）
# ============================================================================
def n_eff_entropy(dist: np.ndarray, eps: float = 1e-12) -> float:
    """
    ========================================================================
    函数功能：计算熵口径的有效专家数（Effective Number of Experts）
    ========================================================================

    【什么是有效专家数？】
    有效专家数（n_eff）用于衡量"实际被使用的专家数量"。

    【给零基础同学的解释】：
    想象你有 60 个专家，但实际使用情况是：
    - 情况1：所有 60 个专家使用频率相同 → n_eff = 60（完全分散）
    - 情况2：只有 3 个专家被频繁使用 → n_eff ≈ 3（高度集中）

    n_eff 就是"等效的均匀使用专家数"。

    【为什么需要 n_eff？】
    1. 衡量负载集中度：n_eff 越小，负载越集中
    2. 比较不同任务：哪个任务使用的专家更多样化
    3. 评估模型容量：是否充分利用了所有专家

    【数学公式】：
    n_eff = exp(H)

    其中：
    - H：熵（自然对数）
    - H = -Σ p_i * ln(p_i)
    - exp：自然指数函数

    【为什么用 exp(H)？】
    因为熵 H 衡量"不确定性"，exp(H) 可以转换为"等效选项数"。

    【示例】：
    假设有 60 个专家，3 种使用分布：

    情况1（完全均匀）：
    所有专家使用频率相同
    p = [1/60, 1/60, ..., 1/60]（60个相同值）

    【逐步计算】：
    1. 计算熵：
       H = -Σ (1/60) * ln(1/60)
       H = -60 * (1/60) * ln(1/60)
       H = -ln(1/60)
       H = ln(60) ≈ 4.094

    2. 计算 n_eff：
       n_eff = exp(4.094) = 60

    【结果解释】：n_eff = 60，所有专家都被均匀使用。

    情况2（高度集中）：
    只有 3 个专家被频繁使用
    p = [0.4, 0.3, 0.3, 0, 0, ..., 0]（前3个有值，其余57个为0）

    【逐步计算】：
    1. 计算熵：
       H = -(0.4*ln(0.4) + 0.3*ln(0.3) + 0.3*ln(0.3))
       H = -(0.4*(-0.916) + 0.3*(-1.204) + 0.3*(-1.204))
       H = -(-0.366 - 0.361 - 0.361)
       H ≈ 1.088

    2. 计算 n_eff：
       n_eff = exp(1.088) ≈ 2.97

    【结果解释】：n_eff ≈ 3，相当于只有 3 个专家被使用。

    情况3（中等集中）：
    10 个专家被使用，但不均匀
    p = [0.2, 0.15, 0.15, 0.1, 0.1, 0.08, 0.08, 0.06, 0.05, 0.03, 0, ..., 0]

    【逐步计算】：
    1. 计算熵：
       H = -(0.2*ln(0.2) + 0.15*ln(0.15) + ... + 0.03*ln(0.03))
       H ≈ 2.15

    2. 计算 n_eff：
       n_eff = exp(2.15) ≈ 8.6

    【结果解释】：n_eff ≈ 8.6，相当于约 9 个专家被均匀使用。

    【给零基础同学的解释】：
    想象你有 60 个员工，要统计"实际工作的员工数"：
    - 如果所有人工作量相同 → 实际工作人数 = 60
    - 如果只有 3 个人干活，其他人摸鱼 → 实际工作人数 ≈ 3
    - 如果 10 个人干活，但工作量不均 → 实际工作人数 ≈ 8-9

    【参数说明】：
    - dist: np.ndarray，专家使用分布（概率或频次）
    - eps: float，防止 log(0) 的小常数

    【返回值】：
    - float，有效专家数，范围 [1, 专家总数]
    """
    # ====================================================================
    # 步骤1：归一化
    # ====================================================================
    """
    【归一化】：
    p = safe_normalize(dist)

    【为什么要归一化？】
    确保分布和为 1（概率分布的定义）。

    【给零基础同学的解释】：
    想象你要把"使用次数"转换为"使用比例"。
    """
    p = safe_normalize(dist)

    # ====================================================================
    # 步骤2：截断，避免 log(0)
    # ====================================================================
    """
    【截断】：
    p = np.clip(p, eps, 1.0)

    【为什么要截断？】
    因为 ln(0) = -∞，会导致计算错误。

    【给零基础同学的解释】：
    想象你要计算 ln(概率)：
    - 如果概率 = 0，ln(0) 会报错
    - 所以我们把 0 替换成一个很小的数
    """
    p = np.clip(p, eps, 1.0)

    # ====================================================================
    # 步骤3：计算熵（自然对数）
    # ====================================================================
    """
    【计算熵】：
    h = -Σ p_i * ln(p_i)

    【代码实现】：
    h = float(-np.sum(p * np.log(p)))

    【注意】：这里用 np.log（自然对数），不是 np.log2。

    【给零基础同学的解释】：
    想象你要计算"不确定性"：
    - 每个专家的"惊讶度" = -ln(概率)
    - 总不确定性 = 加权平均惊讶度
    """
    h = float(-np.sum(p * np.log(p)))

    # ====================================================================
    # 步骤4：计算 n_eff
    # ====================================================================
    """
    【计算 n_eff】：
    n_eff = exp(H)

    【为什么用 exp？】
    因为 exp 是 ln 的反函数，可以把熵转换为"等效数量"。

    【示例】：
    H = ln(60) ≈ 4.094
    n_eff = exp(4.094) = 60

    H = ln(3) ≈ 1.099
    n_eff = exp(1.099) = 3

    【给零基础同学的解释】：
    想象你要把"不确定性"转换为"等效选项数"：
    - 不确定性 = ln(60) → 等效选项数 = 60
    - 不确定性 = ln(3) → 等效选项数 = 3
    """
    return float(np.exp(h))


# ============================================================================
# 核心函数：n_eff_simpson（Simpson 口径的有效专家数）
# ============================================================================
def n_eff_simpson(dist: np.ndarray, eps: float = 1e-12) -> float:
    """
    ========================================================================
    函数功能：计算 Simpson 口径的有效专家数
    ========================================================================

    【什么是 Simpson 口径？】
    Simpson 口径是另一种计算有效专家数的方法，基于"碰撞概率"。

    【给零基础同学的解释】：
    想象你随机抽取两个 token，它们选择同一个专家的概率是多少？
    - 概率高 → 专家使用集中 → n_eff 小
    - 概率低 → 专家使用分散 → n_eff 大

    【数学公式】：
    n_eff = 1 / Σ p_i^2

    其中：
    - p_i：第 i 个专家的使用概率
    - Σ p_i^2：碰撞概率（两次随机抽样选到同一专家的概率）

    【为什么用 1 / Σ p_i^2？】
    因为 Σ p_i^2 衡量"集中度"，取倒数就是"分散度"。

    【示例】：
    假设有 60 个专家，3 种使用分布：

    情况1（完全均匀）：
    p = [1/60, 1/60, ..., 1/60]

    【逐步计算】：
    1. 计算 Σ p_i^2：
       Σ p_i^2 = 60 * (1/60)^2
       Σ p_i^2 = 60 * (1/3600)
       Σ p_i^2 = 1/60

    2. 计算 n_eff：
       n_eff = 1 / (1/60) = 60

    【结果解释】：n_eff = 60，所有专家都被均匀使用。

    情况2（高度集中）：
    p = [0.4, 0.3, 0.3, 0, 0, ..., 0]

    【逐步计算】：
    1. 计算 Σ p_i^2：
       Σ p_i^2 = 0.4^2 + 0.3^2 + 0.3^2
       Σ p_i^2 = 0.16 + 0.09 + 0.09
       Σ p_i^2 = 0.34

    2. 计算 n_eff：
       n_eff = 1 / 0.34 ≈ 2.94

    【结果解释】：n_eff ≈ 2.94，相当于约 3 个专家被使用。

    【Simpson vs 熵口径】：
    - 熵口径：基于信息论，更常用
    - Simpson 口径：基于碰撞概率，更直观
    - 两者通常接近，但不完全相同

    【给零基础同学的解释】：
    想象你要统计"实际工作的员工数"：
    - 方法1（熵）：用"不确定性"估计
    - 方法2（Simpson）：用"碰撞概率"估计
    - 两种方法结果接近，互相验证

    【参数说明】：
    - dist: np.ndarray，专家使用分布
    - eps: float，防止除零的小常数

    【返回值】：
    - float，有效专家数，范围 [1, 专家总数]
    """
    # ====================================================================
    # 步骤1：归一化和截断
    # ====================================================================
    p = safe_normalize(dist)
    p = np.clip(p, eps, 1.0)

    # ====================================================================
    # 步骤2：计算 n_eff
    # ====================================================================
    """
    【计算 n_eff】：
    n_eff = 1 / Σ p_i^2

    【代码实现】：
    return float(1.0 / np.sum(p ** 2))

    【逐步拆解】：
    1. p ** 2：每个概率的平方
    2. np.sum(p ** 2)：求和（碰撞概率）
    3. 1.0 / ...：取倒数（有效数量）

    【给零基础同学的解释】：
    想象你要计算"等效工作人数"：
    - 碰撞概率 = 0.34 → 等效人数 = 1/0.34 ≈ 3
    - 碰撞概率 = 1/60 → 等效人数 = 60
    """
    return float(1.0 / np.sum(p ** 2))


# ============================================================================
# Q6 Part 1 完成
# ============================================================================
# 下一部分将包含：
# - gini_coefficient 函数的详细注释
# - top_n_coverage 函数的详细注释
# - lorenz_curve 函数的详细注释
# ============================================================================
