#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ============================================================================
# Q4 长上下文位置漂移分析脚本 - 详细注释补充（第3部分）
# Hard 分布构建、窗口划分、曲线重采样
# ============================================================================

"""
========================================================================
本部分包含 Q4 的核心函数：
1. build_hard_distribution：构建硬分布（Top-1 频次）
2. split_windows：窗口划分函数
3. resample_curve：曲线重采样函数
========================================================================
"""

# ============================================================================
# 核心函数：build_hard_distribution（硬分布构建）
# ============================================================================
def build_hard_distribution(
    probs_window: np.ndarray,
    idx_window: np.ndarray,
    n_experts: int,
) -> np.ndarray:
    """
    ========================================================================
    函数功能：构建"硬分布"（Hard Distribution）
    ========================================================================

    【什么是硬分布？】
    硬分布只统计每个 token 的 Top-1 专家（概率最高的专家），
    然后计算每个专家被选为 Top-1 的频次。

    【给零基础同学的解释】：
    想象你有一个班级，每个学生要选一门"最喜欢的课"：
    - 学生A：最喜欢数学（Top-1）
    - 学生B：最喜欢物理（Top-1）
    - 学生C：最喜欢数学（Top-1）
    - 学生D：最喜欢化学（Top-1）

    硬分布就是统计"每门课被多少学生选为最喜欢"：
    - 数学：2人（50%）
    - 物理：1人（25%）
    - 化学：1人（25%）

    【Hard vs Soft 的区别】：
    - Hard：只看 Top-1（最喜欢的课）
    - Soft：看所有候选课的喜欢程度（加权平均）

    【为什么需要 Hard 分布？】
    1. 简单直观：只看"最终选择"
    2. 鲁棒性强：不受概率估计误差影响
    3. 与实际路由一致：MoE 模型通常只激活 Top-K 专家

    【数学公式】：
    Hard(e) = (1 / T) * Σ 1[Top1_t = e]

    其中：
    - T：窗口内的 token 数量
    - Top1_t：第 t 个 token 的 Top-1 专家
    - 1[...]：指示函数（条件为真时为1，否则为0）

    【示例】：
    假设窗口内有 4 个 token，60 个专家：

    Token 0：
    - probs = [0.6, 0.3, 0.1]（Top-3 概率）
    - indices = [5, 12, 8]（对应专家ID）
    - Top-1：专家5（概率0.6最大）

    Token 1：
    - probs = [0.5, 0.4, 0.1]
    - indices = [12, 5, 20]
    - Top-1：专家12（概率0.5最大）

    Token 2：
    - probs = [0.7, 0.2, 0.1]
    - indices = [5, 8, 12]
    - Top-1：专家5（概率0.7最大）

    Token 3：
    - probs = [0.8, 0.1, 0.1]
    - indices = [8, 5, 12]
    - Top-1：专家8（概率0.8最大）

    【统计结果】：
    - 专家5：被选为 Top-1 两次 → counts[5] = 2
    - 专家12：被选为 Top-1 一次 → counts[12] = 1
    - 专家8：被选为 Top-1 一次 → counts[8] = 1
    - 其他专家：counts = 0

    【归一化后】：
    - 专家5：2/4 = 0.5（50%）
    - 专家12：1/4 = 0.25（25%）
    - 专家8：1/4 = 0.25（25%）
    - 其他专家：0

    【参数说明】：
    - probs_window: np.ndarray，形状 (T, K)
      * T：窗口内的 token 数量
      * K：Top-K 专家数量（通常是6）
      * 每行是一个 token 的 Top-K 概率
    - idx_window: np.ndarray，形状 (T, K)
      * 每行是一个 token 的 Top-K 专家索引
    - n_experts: int，专家总数（通常是60）

    【返回值】：
    - np.ndarray，形状 (n_experts,)
      * 归一化后的硬分布
      * 每个元素表示该专家被选为 Top-1 的频率
    """
    # ====================================================================
    # 步骤1：初始化计数数组
    # ====================================================================
    """
    【初始化】：
    counts = np.zeros(n_experts, dtype=np.float64)

    【为什么用 float64？】
    因为后续要归一化（除以 token 数量），需要浮点数精度。

    【示例】：
    如果有 60 个专家：
    counts = [0.0, 0.0, ..., 0.0]（60个0）

    【给零基础同学的解释】：
    想象你要统计每门课被选为"最喜欢"的次数：
    - 初始化：每门课的计数都是0
    """
    counts = np.zeros(n_experts, dtype=np.float64)

    # ====================================================================
    # 步骤2：遍历窗口内的每个 token
    # ====================================================================
    """
    【遍历逻辑】：
    for t in range(probs_window.shape[0]):
        # 处理第 t 个 token

    【probs_window.shape[0] 的含义】：
    - probs_window 的形状是 (T, K)
    - shape[0] 就是 T（token 数量）

    【给零基础同学的解释】：
    想象你要统计每个学生的最喜欢的课：
    - 遍历每个学生
    - 找到他/她最喜欢的课
    - 给那门课的计数 +1
    """
    for t in range(probs_window.shape[0]):
        # ================================================================
        # 步骤2.1：提取当前 token 的概率和索引
        # ================================================================
        """
        【提取数据】：
        p = probs_window[t]：第 t 个 token 的 Top-K 概率
        idx = idx_window[t]：第 t 个 token 的 Top-K 专家索引

        【示例】：
        假设 t=0（第一个 token）：
        p = [0.6, 0.3, 0.1]（Top-3 概率）
        idx = [5, 12, 8]（对应专家ID）

        【给零基础同学的解释】：
        想象你要看学生A的选课情况：
        - p：学生A对每门课的喜欢程度（分数）
        - idx：对应的课程编号
        """
        p = probs_window[t]
        idx = idx_window[t]

        # ================================================================
        # 步骤2.2：找到 Top-1 专家（概率最大的位置）
        # ================================================================
        """
        【找 Top-1】：
        top1_k = int(np.argmax(p))

        【np.argmax 的作用】：
        返回数组中最大值的索引位置。

        【示例】：
        p = [0.6, 0.3, 0.1]
        np.argmax(p) = 0（最大值0.6在位置0）

        【为什么不直接用 idx[0]？】
        因为 Top-K 概率不一定是排序好的！
        - 有些数据格式中，Top-K 是按专家ID排序的
        - 有些数据格式中，Top-K 是按概率排序的
        - 为了鲁棒性，我们用 argmax 找最大概率的位置

        【给零基础同学的解释】：
        想象你要找学生A最喜欢的课：
        - 看所有课程的分数：[数学90分, 物理85分, 化学80分]
        - 找最高分的位置：位置0（数学）
        """
        top1_k = int(np.argmax(p))

        # ================================================================
        # 步骤2.3：获取 Top-1 专家的ID
        # ================================================================
        """
        【获取专家ID】：
        expert_id = int(idx[top1_k])

        【逐步拆解】：
        1. top1_k：Top-1 在 Top-K 中的位置（0到K-1）
        2. idx[top1_k]：该位置对应的专家ID
        3. int(...)：转换为整数

        【示例】：
        top1_k = 0（位置0）
        idx = [5, 12, 8]
        expert_id = idx[0] = 5（专家5）

        【给零基础同学的解释】：
        想象你找到了最高分的位置（位置0），
        现在要知道这是哪门课：
        - 位置0对应的课程编号是5（数学）
        """
        expert_id = int(idx[top1_k])

        # ================================================================
        # 步骤2.4：累加计数（带边界检查）
        # ================================================================
        """
        【边界检查】：
        if 0 <= expert_id < n_experts:
            counts[expert_id] += 1.0

        【为什么要检查？】
        防止数据错误导致索引越界：
        - 如果 expert_id < 0：无效专家
        - 如果 expert_id >= n_experts：超出范围

        【累加操作】：
        counts[expert_id] += 1.0：给该专家的计数 +1

        【示例】：
        expert_id = 5
        counts[5] += 1.0（专家5的计数增加1）

        【给零基础同学的解释】：
        想象你要统计每门课被选为"最喜欢"的次数：
        - 学生A最喜欢数学 → 数学的计数 +1
        - 学生B最喜欢物理 → 物理的计数 +1
        - 学生C最喜欢数学 → 数学的计数 +1
        """
        if 0 <= expert_id < n_experts:
            counts[expert_id] += 1.0

    # ====================================================================
    # 步骤3：归一化（转换为频率）
    # ====================================================================
    """
    【归一化】：
    return _safe_normalize(counts)

    【_safe_normalize 的作用】：
    将计数转换为概率分布（和为1）。

    【数学公式】：
    Hard(e) = counts(e) / Σ counts(e')

    【示例】：
    counts = [0, 0, 0, 0, 0, 2, 0, 0, 1, ..., 1, ...]
             （专家5计数2，专家8计数1，专家12计数1）
    总和 = 2 + 1 + 1 = 4

    归一化后：
    Hard[5] = 2/4 = 0.5（50%）
    Hard[8] = 1/4 = 0.25（25%）
    Hard[12] = 1/4 = 0.25（25%）
    其他 = 0

    【给零基础同学的解释】：
    想象你统计完每门课的计数：
    - 数学：2人
    - 物理：1人
    - 化学：1人
    - 总共：4人

    归一化就是转换为百分比：
    - 数学：2/4 = 50%
    - 物理：1/4 = 25%
    - 化学：1/4 = 25%
    """
    return _safe_normalize(counts)


# ============================================================================
# 辅助函数：split_windows（窗口划分）
# ============================================================================
def split_windows(n_tokens: int, window_size: int) -> List[Tuple[int, int]]:
    """
    ========================================================================
    函数功能：把 token 序列切成不重叠的窗口
    ========================================================================

    【什么是窗口划分？】
    把长序列切成多个固定大小的窗口，用于分析"位置漂移"。

    【给零基础同学的解释】：
    想象你有一本500页的书，要分析"前半部分"和"后半部分"的风格差异：
    - 方法1：直接对比前250页和后250页
    - 方法2：把书切成多个小段（每段128页），逐段分析

    窗口划分就是"方法2"，可以更细粒度地观察变化趋势。

    【为什么要窗口划分？】
    1. 细粒度分析：观察专家使用如何随位置变化
    2. 统计稳定性：每个窗口有足够的 token 数量
    3. 可视化友好：可以画出"漂移曲线"

    【数学定义】：
    给定序列长度 T 和窗口大小 W：
    - 窗口数量：N = floor(T / W)
    - 窗口 i：[i*W, (i+1)*W)（左闭右开区间）

    【示例】：
    假设序列长度 T=500，窗口大小 W=128：

    【逐步计算】：
    1. 窗口数量：N = floor(500 / 128) = floor(3.906) = 3
    2. 窗口划分：
       - 窗口0：[0, 128)（token 0-127）
       - 窗口1：[128, 256)（token 128-255）
       - 窗口2：[256, 384)（token 256-383）
    3. 剩余 token：384-500（116个 token）被丢弃

    【为什么丢弃剩余 token？】
    因为剩余部分不足一个完整窗口，统计量不稳定。

    【给零基础同学的解释】：
    想象你要把500页的书切成每段128页：
    - 第1段：第1-128页
    - 第2段：第129-256页
    - 第3段：第257-384页
    - 剩余：第385-500页（不足128页，不分析）

    【参数说明】：
    - n_tokens: int，序列总长度（token 数量）
    - window_size: int，窗口大小（每个窗口的 token 数量）

    【返回值】：
    - List[Tuple[int, int]]，窗口列表
      * 每个元素是 (start, end)，表示窗口的起止位置
      * 例如：[(0, 128), (128, 256), (256, 384)]
    """
    # ====================================================================
    # 步骤1：计算窗口数量
    # ====================================================================
    """
    【计算窗口数量】：
    n_windows = n_tokens // window_size

    【// 的含义】：
    整数除法（向下取整）。

    【示例】：
    n_tokens = 500
    window_size = 128
    n_windows = 500 // 128 = 3（丢弃小数部分）

    【给零基础同学的解释】：
    想象你有500个苹果，每箱装128个：
    - 能装几箱？500 ÷ 128 = 3.906
    - 只能装满3箱（剩余116个不够装一箱）
    """
    n_windows = n_tokens // window_size

    # ====================================================================
    # 步骤2：生成窗口列表
    # ====================================================================
    """
    【生成窗口】：
    for i in range(n_windows):
        start = i * window_size
        end = (i + 1) * window_size
        windows.append((start, end))

    【逐步拆解】：
    1. range(n_windows)：生成 [0, 1, 2, ..., n_windows-1]
    2. start = i * window_size：窗口起始位置
    3. end = (i + 1) * window_size：窗口结束位置
    4. (start, end)：窗口区间（左闭右开）

    【示例】：
    n_windows = 3，window_size = 128

    i=0：
    - start = 0 * 128 = 0
    - end = 1 * 128 = 128
    - 窗口：(0, 128)

    i=1：
    - start = 1 * 128 = 128
    - end = 2 * 128 = 256
    - 窗口：(128, 256)

    i=2：
    - start = 2 * 128 = 256
    - end = 3 * 128 = 384
    - 窗口：(256, 384)

    【给零基础同学的解释】：
    想象你要标记每箱苹果的范围：
    - 第1箱：第1-128个苹果
    - 第2箱：第129-256个苹果
    - 第3箱：第257-384个苹果
    """
    windows = []
    for i in range(n_windows):
        start = i * window_size
        end = (i + 1) * window_size
        windows.append((start, end))

    return windows


# ============================================================================
# 第3部分完成
# ============================================================================
# 下一部分将包含：
# - resample_curve 函数的详细注释
# - mean_ci 函数的详细注释
# - 边界检测函数的详细注释
# ============================================================================
