# ============================================================================
# 导入依赖库
# ============================================================================
import os  # 操作系统接口：用于文件路径操作（如 os.path.join, os.makedirs）
import glob  # 文件名模式匹配：用于查找符合特定模式的文件（如 *.npz）
import numpy as np  # 数值计算库：用于数组操作、统计计算
import argparse  # 命令行参数解析：用于从命令行接收参数（如 --data_dir）
import logging  # 日志记录：用于输出运行时信息、警告、错误
import re  # 正则表达式：用于字符串模式匹配（如提取层名中的数字）
import json  # JSON 编解码：用于读写 JSON 格式的结果文件
import math  # 数学函数：用于数学运算（如 sqrt, log）
from collections import defaultdict  # 默认字典：访问不存在的键时自动创建默认值
import matplotlib.pyplot as plt  # 绘图库：用于生成可视化图表
from sklearn.cluster import KMeans  # K-means 聚类算法（本脚本未使用，可能是历史遗留）
from sklearn.decomposition import PCA  # 主成分分析（本脚本未使用，可能是历史遗留）
from typing import Dict, List  # 类型提示：用于标注函数参数和返回值的类型

# 尝试导入 scipy 的卡方分布函数
# scipy 是科学计算库，chi2 用于卡方检验的 p 值计算
try:
    from scipy.stats import chi2 as scipy_chi2  # 导入卡方分布的累积分布函数
    SCIPY_AVAILABLE = True  # 标记 scipy 可用
except Exception:  # 如果导入失败（如 scipy 未安装）
    SCIPY_AVAILABLE = False  # 标记 scipy 不可用，后续会跳过 p 值计算

# 配置日志系统
# level=logging.INFO：只输出 INFO 及以上级别的日志（INFO, WARNING, ERROR）
# format：日志格式，包含时间戳、日志级别、消息内容
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_data(data_dir: str, max_files: int = -1):
    """
    ========================================================================
    函数功能：加载数据目录下的所有 NPZ 文件，并拼接成统一的数据结构
    ========================================================================

    【功能详解】：
    这个函数负责把分散在硬盘上的很多个小文件（.npz）读取进来，然后拼成一个大的数据块，方便后面分析。

    【给零基础同学的解释】：
    我们的程序在运行时，是一个批次一个批次保存数据的（比如 sample-00001.npz, sample-00002.npz）。
    现在要分析了，得把它们全部读到内存里。

    【参数说明】：
    - data_dir: str 类型，存放 .npz 文件的目录路径（绝对路径或相对路径）
    - max_files: int 类型，默认 -1 表示读取所有文件
                 如果设置为正数（如 10），则只读取前 10 个文件（用于调试）

    【返回值】：
    - all_data: defaultdict(list) 类型的字典，键是数据名称，值是数组列表
                例如：{"tokens_str": [...], "MoE_Gate0_probs": [...], ...}
    """
    # ========================================================================
    # 步骤1：查找并排序所有 NPZ 文件
    # ========================================================================
    # os.path.join(data_dir, "*.npz")：拼接路径，生成 "data_dir/*.npz" 的模式
    # glob.glob(...)：查找所有匹配该模式的文件，返回文件路径列表
    # sorted(...)：对文件路径列表排序，确保按文件名顺序读取（重要！保证可复现性）
    npz_files = sorted(glob.glob(os.path.join(data_dir, "*.npz")))
    
    # ========================================================================
    # 步骤2：限制文件数量（用于调试）
    # ========================================================================
    # 如果 max_files > 0，则只取前 max_files 个文件
    # 例如：max_files=10，则 npz_files[:10] 只取前 10 个文件
    # 这是 Python 的切片语法：list[start:end]，从 start 到 end-1
    if max_files > 0:
        npz_files = npz_files[:max_files]  # 切片操作，只保留前 max_files 个元素

    # ========================================================================
    # 步骤3：输出日志，告诉用户找到了多少个文件
    # ========================================================================
    # f"..." 是 Python 的 f-string 格式化字符串
    # {len(npz_files)} 会被替换为 npz_files 列表的长度（文件数量）
    logging.info(f"Found {len(npz_files)} NPZ files.")

    # ========================================================================
    # 步骤4：创建数据容器（defaultdict）
    # ========================================================================
    # defaultdict(list)：这是一个"默认字典"，非常方便
    # 普通字典：如果访问不存在的键，会报 KeyError 错误
    # 默认字典：如果访问不存在的键，会自动创建一个空列表 []
    #
    # 例如：
    #   普通字典：data = {}; data["key"].append(1)  # 报错！"key" 不存在
    #   默认字典：data = defaultdict(list); data["key"].append(1)  # 正常！自动创建 []
    #
    # 我们用它来分类存放不同层的数据（每层的 probs 和 indices）
    all_data = defaultdict(list)  # 创建一个默认值为空列表的字典
    
    # ========================================================================
    # 步骤5：遍历每个 NPZ 文件，读取并存储数据
    # ========================================================================
    for f in npz_files:  # f 是当前文件的完整路径（字符串）
        try:  # try-except 块：捕获可能的读取错误，避免程序崩溃
            # ================================================================
            # 5.1 读取 NPZ 文件
            # ================================================================
            # np.load(f, allow_pickle=True)：numpy 的文件读取函数
            #   - f：文件路径
            #   - allow_pickle=True：允许读取 Python 对象（如字符串数组）
            #                        NPZ 文件可能包含非数值数据，需要此参数
            # with ... as data：上下文管理器，确保文件读取后自动关闭
            #   - data 是一个类似字典的对象，键是数据名称，值是 numpy 数组
            with np.load(f, allow_pickle=True) as data:
                # ============================================================
                # 5.2 读取 Token 文本（tokens_str）
                # ============================================================
                # data["tokens_str"]：从 NPZ 文件中提取键为 "tokens_str" 的数据
                # tokens_str 是一个字符串数组，存储每个 token 的文本
                # 例如：["The", "quick", "brown", "fox", ...]
                tokens_str = data["tokens_str"]  # 提取 token 文本数组

                # all_data["tokens_str"].extend(tokens_str)：
                #   - all_data["tokens_str"]：访问字典中键为 "tokens_str" 的列表
                #   - .extend(tokens_str)：把 tokens_str 中的所有元素追加到列表末尾
                #   - 注意：extend 是"展开追加"，append 是"整体追加"
                #     例如：[1,2].extend([3,4]) → [1,2,3,4]
                #          [1,2].append([3,4]) → [1,2,[3,4]]
                all_data["tokens_str"].extend(tokens_str)  # 追加 token 文本到总列表
                
                # ============================================================
                # 5.3 遍历 NPZ 文件中的所有键，提取路由数据
                # ============================================================
                # data.keys()：返回 NPZ 文件中所有键的列表
                # 例如：["tokens_str", "layers/MoE_Gate0_out_probs", "layers/MoE_Gate0_out_indices", ...]
                for k in data.keys():  # k 是当前键的名称（字符串）
                    # ========================================================
                    # 5.3.1 过滤：只处理路由概率数据
                    # ========================================================
                    # 我们只关心格式为 "layers/MoE_Gate{N}_out_probs" 的键
                    # 例如：layers/MoE_Gate0_out_probs, layers/MoE_Gate1_out_probs, ...
                    #
                    # k.startswith("layers/")：检查字符串 k 是否以 "layers/" 开头
                    # k.endswith("_out_probs")：检查字符串 k 是否以 "_out_probs" 结尾
                    # and：逻辑与，两个条件都满足才执行 if 块
                    if k.startswith("layers/") and k.endswith("_out_probs"):
                        # ================================================
                        # 5.3.2 解析层名
                        # ================================================
                        # 从键名 "layers/MoE_Gate0_out_probs" 提取层名 "MoE_Gate0"
                        #
                        # k.split("/")：按 "/" 分割字符串
                        #   例如："layers/MoE_Gate0_out_probs".split("/") → ["layers", "MoE_Gate0_out_probs"]
                        # [1]：取分割后的第二个元素（索引从 0 开始）
                        #   结果："MoE_Gate0_out_probs"
                        # .replace("_out_probs", "")：删除后缀 "_out_probs"
                        #   结果："MoE_Gate0"
                        layer_name = k.split("/")[1].replace("_out_probs", "")

                        # ================================================
                        # 5.3.3 读取概率数据（probs）
                        # ================================================
                        # data[k]：从 NPZ 文件中提取键为 k 的数据
                        # probs 是一个 2D numpy 数组，形状为 (n_tokens, K+2)
                        #   - n_tokens：这个批次的 token 数量
                        #   - K+2：Top-K 专家的概率（K 通常是 6，所以是 8 列）
                        # 例如：probs[0] = [0.3, 0.2, 0.1, 0.05, 0.03, 0.02, 0.0, 0.0]
                        #       表示第 0 个 token 的 Top-6 专家概率
                        probs = data[k]  # 形状: (Token数, K+2)
                        
                        # ================================================
                        # 5.3.4 读取专家索引数据（indices）
                        # ================================================
                        # 既然有概率数据 _out_probs，就一定有对应的索引数据 _out_indices
                        # f"layers/{layer_name}_out_indices"：f-string 格式化字符串
                        #   例如：layer_name="MoE_Gate0" → "layers/MoE_Gate0_out_indices"
                        # indices 是一个 2D numpy 数组，形状为 (n_tokens, K+2)
                        #   - 每个元素是专家的 ID（整数，范围 0-59）
                        # 例如：indices[0] = [5, 12, 3, 45, 23, 8, 0, 0]
                        #       表示第 0 个 token 的 Top-6 专家 ID
                        indices = data[f"layers/{layer_name}_out_indices"]  # 形状: (Token数, K+2)

                        # ================================================
                        # 5.3.5 存储数据到 all_data 字典
                        # ================================================
                        # all_data[f"{layer_name}_probs"]：访问字典中的键（如 "MoE_Gate0_probs"）
                        # .append(probs)：把当前文件的 probs 数组追加到列表末尾
                        # 注意：这里用 append（整体追加），不是 extend（展开追加）
                        #       因为 probs 是一个 2D 数组，我们要保持它的结构
                        all_data[f"{layer_name}_probs"].append(probs)  # 追加概率数组
                        all_data[f"{layer_name}_indices"].append(indices)  # 追加索引数组
        # ================================================================
        # 5.4 异常处理：捕获文件读取错误
        # ================================================================
        except Exception as e:  # e 是异常对象，包含错误信息
            # logging.warning：输出警告日志（不会中断程序）
            # f"Error reading {f}: {e}"：格式化字符串，显示出错的文件路径和错误信息
            # 例如："Error reading /path/to/file.npz: File not found"
            logging.warning(f"Error reading {f}: {e}")
            # 注意：这里用 warning 而不是 error，因为单个文件出错不应该导致整个程序崩溃
            # 程序会跳过这个文件，继续处理下一个文件

    # ========================================================================
    # 步骤6：数据合并（拼接）
    # ========================================================================
    # 现在 all_data 里存的是很多个小数组的列表：[array1, array2, array3...]
    # 我们需要把它们拼成一个大数组：big_array
    #
    # 【为什么要拼接？】
    # 因为后续分析需要对"所有 token"做统计，如果数据分散在多个数组里，
    # 每次都要写循环遍历，非常麻烦。拼接后可以直接用 numpy 的向量化操作。
    #
    # 【拼接示例】：
    # 假设有 3 个文件，每个文件 100 个 token：
    #   all_data["MoE_Gate0_probs"] = [array(100,8), array(100,8), array(100,8)]
    # 拼接后：
    #   all_data["MoE_Gate0_probs"] = array(300,8)  # 一个大数组
    for k in all_data:  # k 是字典的键（如 "tokens_str", "MoE_Gate0_probs", ...）
        # ================================================================
        # 6.1 跳过字符串列表（不需要拼接）
        # ================================================================
        if k != "tokens_str":  # tokens_str 是字符串列表，不是 numpy 数组
            try:  # try-except 块：捕获拼接失败的情况
                # ========================================================
                # 6.2 尝试拼接数组
                # ========================================================
                # np.concatenate(all_data[k], axis=0)：
                #   - all_data[k]：数组列表，如 [array1, array2, array3]
                #   - axis=0：沿着第 0 维（行）拼接
                #     例如：array1.shape=(100,8), array2.shape=(100,8)
                #          拼接后：(200,8)
                # 赋值回 all_data[k]：用拼接后的大数组替换原来的列表
                all_data[k] = np.concatenate(all_data[k], axis=0)
            except ValueError as e:  # ValueError：拼接失败（如形状不匹配）
                # ====================================================
                # 6.3 拼接失败的处理
                # ====================================================
                # 【为什么会失败？】
                # 如果不同文件的 top-k 数量不同（如有的是 6，有的是 8），
                # 数组的列数不一致，无法拼接。
                #
                # 【怎么处理？】
                # 保持为列表形式，后续代码会用 iterate_layer_data 函数逐个处理。
                logging.warning(f"Concatenation failed for key {k}: {e}. Keeping as list of arrays.")
                pass  # pass：空语句，什么都不做（保持原样）
            
    # ========================================================================
    # 步骤7：返回加载完成的数据
    # ========================================================================
    return all_data  # 返回字典，包含所有层的 probs 和 indices 数据


# ============================================================================
# 函数：iterate_layer_data（生成器函数）
# ============================================================================
def iterate_layer_data(all_data: Dict[str, any], layer_name: str):
    """
    ========================================================================
    函数功能：逐个 Token 返回 (probs, indices) 数据（生成器）
    ========================================================================

    【什么是生成器？】
    生成器是一种特殊的函数，使用 yield 而不是 return。
    它不会一次性返回所有数据，而是"按需生成"，节省内存。

    【为什么需要这个函数？】
    因为 load_data 函数可能返回两种格式：
    1. 拼接成功：numpy array，形状 (Total_Tokens, K)
    2. 拼接失败：list of arrays，每个元素是一个批次的数据

    这个函数统一处理这两种格式，让后续代码不用关心数据格式。

    【参数说明】：
    - all_data: Dict[str, any]，load_data 返回的数据字典
    - layer_name: str，层名（如 "MoE_Gate0"）

    【返回值】：
    - 生成器，每次 yield 一个 (probs, indices) 元组
      - probs: 1D numpy 数组，形状 (K,)，表示一个 token 的 Top-K 概率
      - indices: 1D numpy 数组，形状 (K,)，表示一个 token 的 Top-K 专家 ID
    """
    # ========================================================================
    # 步骤1：提取指定层的数据
    # ========================================================================
    # f"{layer_name}_probs"：格式化字符串，如 "MoE_Gate0_probs"
    # all_data[...]：从字典中提取对应的数据
    probs_data = all_data[f"{layer_name}_probs"]  # 概率数据
    inds_data = all_data[f"{layer_name}_indices"]  # 索引数据

    # ========================================================================
    # 步骤2：判断数据格式，分情况处理
    # ========================================================================
    # isinstance(probs_data, list)：检查 probs_data 是否是列表类型
    # 如果是列表，说明之前的 concatenate 失败了，数据还是分批存储的
    if isinstance(probs_data, list):  # 情况1：数据是列表（分批存储）
        # ================================================================
        # 情况1的处理：遍历每个批次，再遍历批次内的每个 token
        # ================================================================
        # zip(probs_data, inds_data)：同时遍历两个列表
        #   - probs_data: [batch1_probs, batch2_probs, ...]
        #   - inds_data: [batch1_indices, batch2_indices, ...]
        #   - zip 会配对：(batch1_probs, batch1_indices), (batch2_probs, batch2_indices), ...
        for batch_p, batch_i in zip(probs_data, inds_data):
            # batch_p: 一个批次的概率数据，形状 (Batch_Size, K)
            # batch_i: 一个批次的索引数据，形状 (Batch_Size, K)

            # 再次使用 zip 遍历批次内的每一行（每个 token）
            # zip(batch_p, batch_i)：配对每个 token 的 probs 和 indices
            for p, i in zip(batch_p, batch_i):
                # p: 1D 数组，形状 (K,)，一个 token 的 Top-K 概率
                # i: 1D 数组，形状 (K,)，一个 token 的 Top-K 专家 ID
                # yield: 生成器关键字，返回一个值但不结束函数
                #        下次调用时从这里继续执行
                yield p, i  # 返回 (probs, indices) 元组
                
    # ========================================================================
    # 情况2：数据是 object 类型的 numpy 数组（特殊情况）
    # ========================================================================
    # isinstance(probs_data, np.ndarray)：检查是否是 numpy 数组
    # probs_data.dtype == object：检查数组元素类型是否是 object
    # object 类型数组：每个元素可以是任意 Python 对象（如嵌套数组）
    elif isinstance(probs_data, np.ndarray) and probs_data.dtype == object:
        # 遍历 object 数组中的每个元素
        for batch_p, batch_i in zip(probs_data, inds_data):
            # 检查元素是否是 2D 数组（批次数据）
            if isinstance(batch_p, np.ndarray) and batch_p.ndim >= 2:
                # 如果是 2D 数组，逐行遍历
                for p, i in zip(batch_p, batch_i):
                    yield p, i  # 返回每个 token 的数据
            else:
                # 如果不是 2D 数组，直接返回（可能是单个 token）
                yield batch_p, batch_i

    # ========================================================================
    # 情况3：数据是普通的 numpy 数组（最常见的情况）
    # ========================================================================
    elif isinstance(probs_data, np.ndarray):
        # probs_data 是 2D 数组，形状 (Total_Tokens, K)
        # inds_data 也是 2D 数组，形状 (Total_Tokens, K)
        # zip(probs_data, inds_data)：同时遍历两个数组的每一行
        #   - 第 1 次迭代：(probs_data[0], inds_data[0])
        #   - 第 2 次迭代：(probs_data[1], inds_data[1])
        #   - ...
        for p, i in zip(probs_data, inds_data):
            # p: 1D 数组，形状 (K,)，一个 token 的 Top-K 概率
            # i: 1D 数组，形状 (K,)，一个 token 的 Top-K 专家 ID
            yield p, i  # 返回 (probs, indices) 元组

    # ========================================================================
    # 情况4：未知格式（错误处理）
    # ========================================================================
    else:
        # 如果数据格式不是上述三种情况，说明出现了意外情况
        # logging.error：输出错误日志
        # type(probs_data)：获取数据的类型（如 <class 'list'>）
        logging.error(f"Unknown data format for layer {layer_name}: {type(probs_data)}")
        # raise ValueError：抛出异常，中断程序
        # 这是严重错误，必须停止程序，让开发者检查数据格式
        raise ValueError(f"Unknown data format for {layer_name}")

# ============================================================================
# 函数：analyze_expert_load（专家负载分析）
# ============================================================================
def analyze_expert_load(all_data: Dict[str, np.ndarray], output_dir: str):
    """
    ========================================================================
    函数功能：分析每个专家的负载分布，并生成可视化图表
    ========================================================================

    【什么是负载？】
    负载（Load）表示每个专家承担的工作量。
    我们用"概率和"来衡量：专家被选中的所有概率累加起来。

    【计算示例】：
    假设专家 A 在 3 个 token 中被选中，概率分别是 0.8, 0.1, 0.5
    那么专家 A 的负载 = 0.8 + 0.1 + 0.5 = 1.4

    【为什么要分析负载？】
    - 如果负载分布不均匀，说明某些专家"过载"，某些专家"闲置"
    - 这会影响模型的效率和泛化能力

    【参数说明】：
    - all_data: Dict[str, np.ndarray]，load_data 返回的数据字典
    - output_dir: str，输出目录路径，用于保存图表

    【返回值】：
    - 无返回值，直接保存图表到 output_dir
    """
    # ========================================================================
    # 步骤1：创建输出目录
    # ========================================================================
    # os.makedirs(output_dir, exist_ok=True)：创建目录
    #   - output_dir：目录路径
    #   - exist_ok=True：如果目录已存在，不报错（否则会抛出 FileExistsError）
    os.makedirs(output_dir, exist_ok=True)
    
    # ========================================================================
    # 步骤2：提取所有层名
    # ========================================================================
    # 【列表推导式】：这是 Python 的一种简洁语法，用于从列表生成新列表
    # 语法：[表达式 for 变量 in 可迭代对象 if 条件]
    #
    # 【逐步拆解】：
    # 1. all_data.keys()：获取字典的所有键
    #    例如：["tokens_str", "MoE_Gate0_probs", "MoE_Gate0_indices", "MoE_Gate1_probs", ...]
    # 2. if k.endswith("_probs")：过滤条件，只保留以 "_probs" 结尾的键
    #    例如：["MoE_Gate0_probs", "MoE_Gate1_probs", ...]
    # 3. k.replace("_probs", "")：对每个键删除 "_probs" 后缀
    #    例如：["MoE_Gate0", "MoE_Gate1", ...]
    layer_names = [k.replace("_probs", "") for k in all_data.keys() if k.endswith("_probs")]

    # ========================================================================
    # 步骤3：遍历每一层，计算负载分布
    # ========================================================================
    for layer in layer_names:  # layer 是层名，如 "MoE_Gate0"
        # ================================================================
        # 3.1 提取当前层的数据
        # ================================================================
        # f"{layer}_probs"：格式化字符串，如 "MoE_Gate0_probs"
        probs = all_data[f"{layer}_probs"]     # 概率数组，形状 (n_tokens, K)
        indices = all_data[f"{layer}_indices"] # 索引数组，形状 (n_tokens, K)

        # ================================================================
        # 3.2 计算专家总数
        # ================================================================
        # 【如何计算专家数量？】
        # 专家 ID 从 0 开始编号，所以最大 ID + 1 = 专家总数
        # 例如：ID 范围是 0-59，则有 60 个专家
        #
        # np.max(indices)：找出 indices 数组中的最大值
        # int(...)：转换为整数类型
        # + 1：因为 ID 从 0 开始，所以要加 1
        # if len(indices) > 0 else 0：三元表达式，如果 indices 为空，返回 0
        num_experts = int(np.max(indices) + 1) if len(indices) > 0 else 0

        # ================================================================
        # 3.3 初始化负载数组
        # ================================================================
        # np.zeros(num_experts)：创建一个全 0 的数组，长度为专家数量
        # 例如：num_experts=60，则创建 [0, 0, 0, ..., 0]（60个0）
        # 这个数组用来累加每个专家的负载
        load_distribution = np.zeros(num_experts)

        # ================================================================
        # 3.4 展平数组（Flatten）
        # ================================================================
        # 【什么是 flatten？】
        # flatten() 把多维数组"压扁"成一维数组
        #
        # 【示例】：
        # 原始数组：[[1, 2], [3, 4], [5, 6]]  # 形状 (3, 2)
        # flatten后：[1, 2, 3, 4, 5, 6]       # 形状 (6,)
        #
        # 【为什么要 flatten？】
        # 因为后面要用 np.add.at 函数，它需要一维数组作为索引
        # 这样可以避免写两层循环，提高效率
        flat_indices = indices.flatten()  # 展平索引数组
        flat_probs = probs.flatten()      # 展平概率数组

        # ================================================================
        # 3.5 累加负载（核心步骤）
        # ================================================================
        # np.add.at(a, indices, b)：这是一个非常强大的函数！
        #
        # 【功能】：在数组 a 的 indices 位置，累加 b 的值
        # 【语法】：np.add.at(目标数组, 索引数组, 值数组)
        #
        # 【关键特性】：自动处理重复索引！
        # 如果 indices 中有重复的 ID，会自动累加到同一个位置
        #
        # 【示例】：
        # load_distribution = [0, 0, 0]  # 3个专家
        # flat_indices = [0, 1, 0, 2]    # 专家0被选2次，专家1和2各1次
        # flat_probs = [0.5, 0.3, 0.2, 0.1]
        # 执行 np.add.at(load_distribution, flat_indices, flat_probs) 后：
        # load_distribution = [0.7, 0.3, 0.1]  # 专家0: 0.5+0.2=0.7
        np.add.at(load_distribution, flat_indices, flat_probs)

        # ================================================================
        # 3.6 归一化负载（转换为百分比）
        # ================================================================
        # 【什么是归一化？】
        # 归一化就是把绝对数值转换为相对比例（百分比）
        # 这样更容易理解：专家 A 承担了 5% 的工作量
        #
        # 【计算方法】：
        # 每个专家的负载 / 总负载 = 该专家的负载占比
        #
        # np.sum(load_distribution)：计算所有专家的负载总和
        total_load = np.sum(load_distribution)

        # 如果总负载 > 0，则进行归一化
        # 注意：必须检查 total_load > 0，否则会除以 0 导致错误
        if total_load > 0:
            # /= 是复合赋值运算符，等价于：load_distribution = load_distribution / total_load
            # 这会把数组中的每个元素都除以 total_load
            load_distribution /= total_load
            
        # ================================================================
        # 3.7 绘制负载分布图
        # ================================================================
        # 【使用 matplotlib 绘图】
        # matplotlib 是 Python 最常用的绘图库
        #
        # plt.figure(figsize=(10, 6))：创建一个新图表
        #   - figsize=(宽, 高)：图表尺寸，单位是英寸
        #   - (10, 6) 表示宽 10 英寸，高 6 英寸
        plt.figure(figsize=(10, 6))

        # plt.bar(x, height)：绘制柱状图
        #   - x：横轴坐标（专家 ID）
        #   - height：柱子高度（负载值）
        # range(num_experts)：生成 0 到 num_experts-1 的整数序列
        #   例如：num_experts=60，则生成 [0, 1, 2, ..., 59]
        plt.bar(range(num_experts), load_distribution)  # x轴是专家ID，y轴是负载
        # plt.title()：设置图表标题
        # f"Expert Load Distribution - {layer}"：格式化字符串
        #   例如：layer="MoE_Gate0" → "Expert Load Distribution - MoE_Gate0"
        plt.title(f"Expert Load Distribution - {layer}")

        # plt.xlabel()：设置横轴标签
        plt.xlabel("Expert ID")

        # plt.ylabel()：设置纵轴标签
        # "Weighted Load (Probability Mass)"：加权负载（概率质量）
        plt.ylabel("Weighted Load (Probability Mass)")

        # ================================================================
        # 3.8 保存图表并释放内存
        # ================================================================
        # plt.savefig()：保存图表为图片文件
        # os.path.join(output_dir, f"{layer}_load.png")：拼接文件路径
        #   例如：output_dir="/output", layer="MoE_Gate0"
        #        → "/output/MoE_Gate0_load.png"
        plt.savefig(os.path.join(output_dir, f"{layer}_load.png"))

        # plt.close()：关闭当前图表，释放内存
        # 【为什么要关闭？】
        # 如果不关闭，matplotlib 会在内存中保留所有图表
        # 当处理很多层时，会导致内存占用过高，甚至内存溢出
        plt.close()  # 画完了必须关闭，释放内存

        # ================================================================
        # 3.9 输出日志
        # ================================================================
        # logging.info()：输出信息级别的日志
        # f"Saved load plot for {layer}"：格式化字符串，告诉用户图表已保存
        logging.info(f"Saved load plot for {layer}")


# ============================================================================
# 函数：_sorted_layer_names（按层序号排序层名）
# ============================================================================
def _sorted_layer_names(all_data: Dict[str, any]) -> List[str]:
    """
    ========================================================================
    函数功能：提取并排序所有层名，按照层序号（Gate0, Gate1, ...）排序
    ========================================================================

    【为什么需要排序？】
    字典的键是无序的，直接遍历可能得到乱序的层名（如 Gate2, Gate0, Gate1）
    我们需要按照层的顺序（Gate0 → Gate1 → Gate2 → ...）处理数据

    【参数】：
    - all_data: Dict[str, any]，load_data 返回的数据字典

    【返回值】：
    - List[str]，排序后的层名列表，如 ["MoE_Gate0", "MoE_Gate1", ...]
    """
    # 提取所有层名（与 analyze_expert_load 中的逻辑相同）
    layer_names = [k.replace("_probs", "") for k in all_data.keys() if k.endswith("_probs")]

    # ========================================================================
    # 定义排序键函数：从层名中提取数字
    # ========================================================================
    def layer_id(name: str) -> int:
        """
        从层名中提取层序号，用于排序

        【示例】：
        - "MoE_Gate0" → 0
        - "MoE_Gate12" → 12
        - "SomeOtherLayer" → 10^9（非常大的数，排到最后）
        """
        # re.search(pattern, string)：在字符串中搜索正则表达式模式
        # r"Gate(\d+)$"：正则表达式模式
        #   - r"..."：原始字符串（raw string），反斜杠不需要转义
        #   - Gate：字面匹配 "Gate"
        #   - (\d+)：捕获组，匹配一个或多个数字
        #     - \d：匹配任意数字（0-9）
        #     - +：匹配一次或多次
        #     - ()：捕获组，可以通过 .group(1) 提取
        #   - $：字符串结尾（确保 "Gate" 后面只有数字）
        m = re.search(r"Gate(\d+)$", name)

        # 如果匹配成功，提取数字；否则返回一个很大的数（排到最后）
        # m.group(1)：提取第一个捕获组（括号内的内容）
        # int(...)：转换为整数
        # 10**9：10的9次方，一个很大的数
        return int(m.group(1)) if m else 10**9

    # ========================================================================
    # 使用自定义键函数排序
    # ========================================================================
    # sorted(list, key=function)：对列表排序
    #   - layer_names：要排序的列表
    #   - key=layer_id：排序键函数，对每个元素调用 layer_id() 获取排序值
    # 例如：["MoE_Gate2", "MoE_Gate0", "MoE_Gate1"]
    #      → [0, 2, 1]（调用 layer_id 后的值）
    #      → ["MoE_Gate0", "MoE_Gate1", "MoE_Gate2"]（排序后）
    return sorted(layer_names, key=layer_id)


# ============================================================================
# 函数：_gini（计算基尼系数）
# ============================================================================
def _gini(x: np.ndarray) -> float:
    """
    ========================================================================
    函数功能：计算基尼系数（Gini Coefficient）
    ========================================================================

    【什么是基尼系数？】
    基尼系数用于衡量分布的不均匀程度
    - 0：完全均匀（所有专家负载相同）
    - 1：完全不均匀（所有负载集中在一个专家）

    【参数】：
    - x: np.ndarray，数值数组（如专家负载分布）

    【返回值】：
    - float，基尼系数，范围 [0, 1]
    """
    # 转换为 float64 类型的数组（确保精度）
    x = np.asarray(x, dtype=np.float64)

    # 如果数组为空，返回 0
    if x.size == 0:
        return 0.0

    # 计算数组总和
    s = float(np.sum(x))

    # 如果总和 <= 0，返回 0（避免除以 0）
    if s <= 0:
        return 0.0

    # 对数组排序（从小到大）
    x = np.sort(x)

    # 获取数组长度
    n = x.size

    # 生成索引数组 [1, 2, 3, ..., n]
    # np.arange(start, stop)：生成从 start 到 stop-1 的整数序列
    idx = np.arange(1, n + 1, dtype=np.float64)

    # 计算基尼系数
    # 公式：G = (2 * Σ(i * x_i) / (n * Σx_i)) - (n+1)/n
    return float((2.0 * np.sum(idx * x) / (n * s)) - (n + 1.0) / n)


# ============================================================================
# 函数：_effective_n（计算有效专家数）
# ============================================================================
def _effective_n(p: np.ndarray) -> float:
    """
    ========================================================================
    函数功能：计算有效专家数（Effective Number）
    ========================================================================

    【什么是有效专家数？】
    有效专家数表示"等效均匀分布的专家数量"
    - 如果 60 个专家负载完全均匀，有效专家数 = 60
    - 如果只有 1 个专家承担所有负载，有效专家数 = 1
    - 如果 10 个专家平均分担负载，其他 50 个闲置，有效专家数 ≈ 10

    【计算公式】：
    n_eff = 1 / Σ(p_i^2)  （Simpson 指数的倒数）

    【参数】：
    - p: np.ndarray，概率分布数组（如专家负载分布）

    【返回值】：
    - float，有效专家数
    """
    # 转换为 float64 类型的数组（确保精度）
    p = np.asarray(p, dtype=np.float64)

    # 计算数组总和
    s = float(np.sum(p))

    # 如果总和 <= 0，返回 0（避免除以 0）
    if s <= 0:
        return 0.0

    # 归一化：确保概率和为 1
    p = p / s

    # 计算分母：Σ(p_i^2)
    # p * p：逐元素平方
    # np.sum(...)：求和
    denom = float(np.sum(p * p))

    # 如果分母 <= 0，返回 0（避免除以 0）
    if denom <= 0:
        return 0.0

    # 返回有效专家数：1 / Σ(p_i^2)
    return float(1.0 / denom)


# ============================================================================
# 函数：_entropy（计算熵）
# ============================================================================
def _entropy(p: np.ndarray) -> float:
    """
    ========================================================================
    函数功能：计算熵（Entropy）
    ========================================================================

    【什么是熵？】
    熵用于衡量分布的不确定性或混乱程度
    - 熵越高：分布越均匀，不确定性越大
    - 熵越低：分布越集中，不确定性越小

    【计算公式】：
    H = -Σ(p_i * log(p_i))  （自然对数）

    【参数】：
    - p: np.ndarray，概率分布数组

    【返回值】：
    - float，熵值
    """
    # 转换为 float64 类型的数组（确保精度）
    p = np.asarray(p, dtype=np.float64)

    # 计算数组总和
    s = float(np.sum(p))

    # 如果总和 <= 0，返回 0（避免除以 0）
    if s <= 0:
        return 0.0

    # 归一化：确保概率和为 1
    p = p / s

    # 裁剪概率值：避免 log(0) 导致的数值错误
    # np.clip(array, min, max)：将数组值限制在 [min, max] 范围内
    # 1e-12：非常小的正数（0.000000000001）
    p = np.clip(p, 1e-12, 1.0)

    # 计算熵：-Σ(p_i * log(p_i))
    # np.log(p)：自然对数
    # p * np.log(p)：逐元素相乘
    # -np.sum(...)：求和并取负
    return float(-np.sum(p * np.log(p)))


# ============================================================================
# 函数：_mi_from_joint（从联合分布计算互信息）
# ============================================================================
def _mi_from_joint(joint: np.ndarray) -> float:
    """
    ========================================================================
    函数功能：从联合分布矩阵计算互信息（Mutual Information）
    ========================================================================

    【什么是互信息？】
    互信息（MI）衡量两个随机变量之间的"相关程度"
    - MI = 0：完全独立（知道 X 不能帮助预测 Y）
    - MI > 0：存在相关性（知道 X 可以减少对 Y 的不确定性）

    【在本项目中的应用】
    用于衡量相邻层的专家选择是否相关：
    - X = 第 i 层的 Top-1 专家 ID
    - Y = 第 i+1 层的 Top-1 专家 ID
    - joint[x, y] = 同时选择专家 x 和专家 y 的 token 数量

    【计算公式】
    MI(X;Y) = Σ Σ p(x,y) * log(p(x,y) / (p(x) * p(y)))
    其中：
    - p(x,y)：联合概率（joint 矩阵归一化后）
    - p(x)：边缘概率（X 的分布）
    - p(y)：边缘概率（Y 的分布）

    【参数】
    - joint: np.ndarray，联合频次矩阵，形状 (n_experts, n_experts)
             joint[i, j] 表示"第 i 层选专家 i，第 i+1 层选专家 j"的次数

    【返回值】
    - float，互信息值（自然对数，单位是 nats）
    """
    # ========================================================================
    # 步骤1：转换为 float64 类型（确保精度）
    # ========================================================================
    # np.asarray：转换为 numpy 数组（如果已经是数组则不复制）
    # dtype=np.float64：指定数据类型为 64 位浮点数（双精度）
    joint = np.asarray(joint, dtype=np.float64)

    # ========================================================================
    # 步骤2：计算总频次（用于归一化）
    # ========================================================================
    # joint.sum()：对整个矩阵求和，得到总 token 数
    # float(...)：转换为 Python 标量（避免 numpy 类型）
    total = float(joint.sum())

    # ========================================================================
    # 步骤3：边界检查（避免除以 0）
    # ========================================================================
    # 如果总频次 <= 0，说明没有数据，返回 0
    if total <= 0:
        return 0.0

    # ========================================================================
    # 步骤4：归一化为联合概率分布
    # ========================================================================
    # pxy[i, j] = joint[i, j] / total
    # 这样 pxy 的所有元素之和为 1，成为概率分布
    pxy = joint / total

    # ========================================================================
    # 步骤5：计算边缘概率分布
    # ========================================================================
    # 【什么是边缘概率？】
    # 从联合分布中"边缘化"掉一个变量，得到单变量的分布
    # 例如：p(x) = Σ_y p(x, y)

    # pxy.sum(axis=1, keepdims=True)：沿着列方向求和
    #   - axis=1：对每一行求和（固定行索引，遍历列）
    #   - keepdims=True：保持维度，结果形状为 (n_experts, 1)
    #   - 结果：px[i, 0] = Σ_j pxy[i, j] = p(第 i 层选专家 i)
    px = pxy.sum(axis=1, keepdims=True)

    # pxy.sum(axis=0, keepdims=True)：沿着行方向求和
    #   - axis=0：对每一列求和（固定列索引，遍历行）
    #   - keepdims=True：保持维度，结果形状为 (1, n_experts)
    #   - 结果：py[0, j] = Σ_i pxy[i, j] = p(第 i+1 层选专家 j)
    py = pxy.sum(axis=0, keepdims=True)

    # ========================================================================
    # 步骤6：计算互信息（核心公式）
    # ========================================================================
    # 【公式推导】
    # MI = Σ Σ p(x,y) * log(p(x,y) / (p(x) * p(y)))
    #    = Σ Σ p(x,y) * [log(p(x,y)) - log(p(x)) - log(p(y))]

    # with np.errstate(divide="ignore", invalid="ignore")：
    #   - 临时禁用除零警告和无效值警告
    #   - 因为 log(0) 会产生 -inf，但我们会用 np.nansum 忽略它
    with np.errstate(divide="ignore", invalid="ignore"):
        # np.log(pxy + 1e-12)：计算 log(p(x,y))
        #   - 加 1e-12 是为了避免 log(0)（虽然后面会用 nansum 处理）
        # np.log(px + 1e-12)：计算 log(p(x))
        # np.log(py + 1e-12)：计算 log(p(y))
        # pxy * (...)：逐元素相乘，得到每个 (x, y) 对的贡献
        # np.nansum(...)：对所有元素求和，忽略 NaN 值
        #   - NaN 来自 0 * log(0) = 0 * (-inf) = NaN
        #   - 数学上 lim_{p→0} p*log(p) = 0，所以忽略 NaN 是正确的
        mi = np.nansum(pxy * (np.log(pxy + 1e-12) - np.log(px + 1e-12) - np.log(py + 1e-12)))

    # ========================================================================
    # 步骤7：返回互信息值
    # ========================================================================
    # float(mi)：转换为 Python 标量
    return float(mi)


# ============================================================================
# 函数：_chi2_from_joint（从联合分布计算卡方统计量）
# ============================================================================
def _chi2_from_joint(joint: np.ndarray):
    """
    ========================================================================
    函数功能：从联合频次矩阵计算卡方统计量（Chi-Square Statistic）
    ========================================================================

    【什么是卡方检验？】
    卡方检验用于判断两个分类变量是否独立
    - 零假设 H0：两个变量独立（相邻层专家选择无关）
    - 备择假设 H1：两个变量相关（相邻层专家选择有关）

    【计算原理】
    比较"观察频次"与"期望频次"的差异：
    - 观察频次：实际数据中的 joint[i, j]
    - 期望频次：如果两层独立，理论上应该出现的频次
    - χ² = Σ (观察 - 期望)² / 期望

    【在本项目中的应用】
    与互信息互补，提供另一个"层间相关性"的证据
    - χ² 值越大：相关性越强
    - p 值越小：越有把握拒绝"独立假设"

    【参数】
    - joint: np.ndarray，联合频次矩阵，形状 (n_experts, n_experts)

    【返回值】
    - chi2: float，卡方统计量
    - p_val: float 或 None，p 值（需要 scipy）
    - df: int，自由度 = (行数-1) * (列数-1)
    """
    # ========================================================================
    # 步骤1：转换为 float64 类型
    # ========================================================================
    joint = np.asarray(joint, dtype=np.float64)

    # ========================================================================
    # 步骤2：计算总频次
    # ========================================================================
    total = float(joint.sum())

    # ========================================================================
    # 步骤3：边界检查
    # ========================================================================
    # 如果没有数据，返回 (0.0, None, 0)
    if total <= 0:
        return 0.0, None, 0

    # ========================================================================
    # 步骤4：计算行和与列和（边缘频次）
    # ========================================================================
    # row[i] = Σ_j joint[i, j]：第 i 层选专家 i 的总次数
    # axis=1：沿列方向求和（对每一行求和）
    row = joint.sum(axis=1)

    # col[j] = Σ_i joint[i, j]：第 i+1 层选专家 j 的总次数
    # axis=0：沿行方向求和（对每一列求和）
    col = joint.sum(axis=0)

    # ========================================================================
    # 步骤5：计算期望频次矩阵
    # ========================================================================
    # 【独立性假设下的期望频次】
    # 如果两层独立，则：
    # expected[i, j] = (row[i] * col[j]) / total
    #                = p(第i层选i) * p(第i+1层选j) * total
    #
    # np.outer(row, col)：外积，生成矩阵
    #   - outer[i, j] = row[i] * col[j]
    #   - 形状：(n_experts, n_experts)
    expected = np.outer(row, col) / total

    # ========================================================================
    # 步骤6：过滤有效单元格（避免除以 0）
    # ========================================================================
    # valid[i, j] = True 当且仅当 expected[i, j] > 0
    # 这样可以避免在计算 (观察-期望)²/期望 时除以 0
    valid = expected > 0

    # ========================================================================
    # 步骤7：计算卡方统计量
    # ========================================================================
    # 【公式】χ² = Σ (O - E)² / E
    # 其中 O = 观察频次，E = 期望频次
    #
    # (joint - expected) ** 2：逐元素计算 (O - E)²
    # [...][valid]：只保留 valid=True 的单元格
    # / expected[valid]：除以对应的期望频次
    # np.sum(...)：对所有有效单元格求和
    chi2 = float(np.sum(((joint - expected) ** 2)[valid] / expected[valid]))

    # ========================================================================
    # 步骤8：计算自由度
    # ========================================================================
    # 【自由度公式】df = (行数 - 1) * (列数 - 1)
    # joint.shape[0]：行数（第 i 层的专家数）
    # joint.shape[1]：列数（第 i+1 层的专家数）
    # 通常两者相等（都是 60）
    df = int((joint.shape[0] - 1) * (joint.shape[1] - 1))

    # ========================================================================
    # 步骤9：计算 p 值（需要 scipy）
    # ========================================================================
    # 【什么是 p 值？】
    # p 值表示"在零假设（独立）成立的情况下，观察到当前或更极端结果的概率"
    # - p < 0.05：通常认为有显著相关性
    # - p < 0.01：强相关性
    # - p < 0.001：非常强的相关性

    p_val = None  # 默认值（如果 scipy 不可用）

    # SCIPY_AVAILABLE：全局变量，标记 scipy 是否成功导入
    # df > 0：确保自由度有效
    if SCIPY_AVAILABLE and df > 0:
        try:
            # scipy_chi2.sf(chi2, df)：卡方分布的生存函数
            #   - sf = survival function = 1 - CDF
            #   - CDF(x) = P(X ≤ x)
            #   - sf(x) = P(X > x)
            # 这里计算的是"χ² 值大于等于观察值的概率"
            p_val = float(scipy_chi2.sf(chi2, df))
        except Exception:
            # 如果计算失败（极端情况），保持 p_val = None
            p_val = None

    # ========================================================================
    # 步骤10：返回三元组
    # ========================================================================
    # 返回：(卡方统计量, p值, 自由度)
    return chi2, p_val, df


# ============================================================================
# 函数：_cramers_v（计算 Cramér's V 系数）
# ============================================================================
def _cramers_v(chi2: float, n: float, r: int, c: int) -> float:
    """
    ========================================================================
    函数功能：计算 Cramér's V 系数（效应量）
    ========================================================================

    【什么是 Cramér's V？】
    Cramér's V 是卡方统计量的"标准化版本"，用于衡量关联强度
    - 范围：[0, 1]
    - 0：完全独立
    - 1：完全关联

    【为什么需要 Cramér's V？】
    卡方统计量 χ² 受样本量影响：
    - 样本量大 → χ² 容易很大（即使关联很弱）
    - 样本量小 → χ² 容易很小（即使关联很强）
    Cramér's V 消除了样本量的影响，可以跨数据集比较

    【计算公式】
    V = sqrt(χ² / (n * min(r-1, c-1)))
    其中：
    - χ²：卡方统计量
    - n：样本总数
    - r：行数（第 i 层的专家数）
    - c：列数（第 i+1 层的专家数）

    【参数】
    - chi2: float，卡方统计量
    - n: float，样本总数
    - r: int，行数
    - c: int，列数

    【返回值】
    - float，Cramér's V 系数，范围 [0, 1]
    """
    # ========================================================================
    # 步骤1：边界检查（样本数）
    # ========================================================================
    # 如果样本数 <= 0，无法计算，返回 0
    if n <= 0:
        return 0.0

    # ========================================================================
    # 步骤2：计算分母
    # ========================================================================
    # 【公式推导】
    # V = sqrt(χ² / (n * φ))
    # 其中 φ = min(r-1, c-1) 是"有效自由度"
    #
    # max(1, min(r - 1, c - 1))：
    #   - min(r - 1, c - 1)：取行列自由度的较小值
    #   - max(1, ...)：确保至少为 1（避免除以 0）
    # n * ...：乘以样本数
    denom = n * max(1, min(r - 1, c - 1))

    # ========================================================================
    # 步骤3：边界检查（分母）
    # ========================================================================
    # 如果分母 <= 0（理论上不应该发生），返回 0
    if denom <= 0:
        return 0.0

    # ========================================================================
    # 步骤4：计算 Cramér's V
    # ========================================================================
    # math.sqrt(chi2 / denom)：开平方根
    # float(...)：转换为 Python 标量
    return float(math.sqrt(chi2 / denom))


# ============================================================================
# 函数：_run_lengths（计算连续段长度）
# ============================================================================
def _run_lengths(seq: np.ndarray) -> List[int]:
    """
    ========================================================================
    函数功能：计算序列中连续相同值的段长度（Run Length Encoding）
    ========================================================================

    【什么是 Run Length？】
    Run Length 是指序列中"连续相同值"的长度
    例如：[5, 5, 5, 12, 12, 5, 5] → [3, 2, 2]
    - 前 3 个都是 5 → 长度 3
    - 接下来 2 个都是 12 → 长度 2
    - 最后 2 个都是 5 → 长度 2

    【在本项目中的应用】
    用于分析"专家切换频率"：
    - seq 是 Top-1 专家 ID 序列（沿 token 维度）
    - run_lengths 越长 → 专家越"稳定"（不频繁切换）
    - run_lengths 越短 → 专家越"跳跃"（频繁切换）

    【参数】
    - seq: np.ndarray，1D 数组，表示 Top-1 专家 ID 序列

    【返回值】
    - List[int]，连续段长度列表
    """
    # ========================================================================
    # 步骤1：边界检查（空序列）
    # ========================================================================
    # seq.size：数组元素个数
    # 如果序列为空，返回空列表
    if seq.size == 0:
        return []

    # ========================================================================
    # 步骤2：初始化状态变量
    # ========================================================================
    # runs：存储所有连续段的长度
    runs = []

    # cur：当前段的值（初始化为第一个元素）
    # seq[0]：取序列的第一个元素
    cur = seq[0]

    # length：当前段的长度（初始化为 1）
    length = 1

    # ========================================================================
    # 步骤3：遍历序列（从第二个元素开始）
    # ========================================================================
    # seq[1:]：切片，从索引 1 到末尾（跳过第一个元素）
    # for v in ...：遍历每个元素
    for v in seq[1:]:
        # ================================================================
        # 3.1 判断是否与当前段相同
        # ================================================================
        if v == cur:
            # 如果相同，当前段长度 +1
            length += 1
        else:
            # ================================================================
            # 3.2 如果不同，说明当前段结束
            # ================================================================
            # 保存当前段的长度
            runs.append(length)

            # 更新当前段的值为新值
            cur = v

            # 重置长度为 1（新段的第一个元素）
            length = 1

    # ========================================================================
    # 步骤4：保存最后一段
    # ========================================================================
    # 循环结束后，最后一段还没有被保存，需要手动添加
    runs.append(length)

    # ========================================================================
    # 步骤5：返回结果
    # ========================================================================
    return runs


# ============================================================================
# 函数：_get_sample_item（从容器中提取指定索引的样本）
# ============================================================================
def _get_sample_item(container: any, sample_idx: int):
    """
    ========================================================================
    函数功能：从不同格式的容器中提取指定索引的样本
    ========================================================================

    【为什么需要这个函数？】
    因为数据可能以两种格式存储：
    1. list of arrays：[array1, array2, array3, ...]
       - 每个 array 是一个批次的数据
       - 需要"跨批次"定位到全局索引
    2. object array：np.array([sample1, sample2, ...], dtype=object)
       - 每个元素是一个样本
       - 直接索引即可

    【参数】
    - container: any，数据容器（list 或 object array）
    - sample_idx: int，全局样本索引

    【返回值】
    - 指定索引的样本数据

    【异常】
    - IndexError：索引超出范围
    - TypeError：容器类型不支持
    """
    # ========================================================================
    # 情况1：容器是列表（list of arrays）
    # ========================================================================
    if isinstance(container, list):
        # ================================================================
        # 跨批次定位逻辑
        # ================================================================
        # 【示例】
        # container = [array(100,8), array(50,8), array(80,8)]
        # sample_idx = 120
        #
        # 第 1 次循环：idx=120, n=100, idx >= n → idx = 120-100 = 20
        # 第 2 次循环：idx=20, n=50, idx < n → 返回 container[1][20]

        # idx：当前剩余索引（初始化为全局索引）
        idx = sample_idx

        # 遍历每个批次
        for arr in container:
            # n：当前批次的样本数
            n = len(arr)

            # 如果剩余索引 < 当前批次大小，说明目标在这个批次里
            if idx < n:
                # 返回当前批次的第 idx 个样本
                return arr[idx]

            # 否则，减去当前批次大小，继续查找下一个批次
            idx -= n

        # 如果遍历完所有批次还没找到，说明索引超出范围
        # raise IndexError：抛出索引错误异常
        raise IndexError(sample_idx)

    # ========================================================================
    # 情况2：容器是 object 类型的 numpy 数组
    # ========================================================================
    # isinstance(container, np.ndarray)：检查是否是 numpy 数组
    # container.dtype == object：检查数组元素类型是否是 object
    if isinstance(container, np.ndarray) and container.dtype == object:
        # 直接索引返回（object 数组的每个元素就是一个样本）
        return container[sample_idx]

    # ========================================================================
    # 情况3：容器是普通的 numpy 数组（非 object 类型）
    # ========================================================================
    if isinstance(container, np.ndarray):
        # 直接索引返回
        return container[sample_idx]

    # ========================================================================
    # 情况4：不支持的容器类型
    # ========================================================================
    # raise TypeError：抛出类型错误异常
    # type(container)：获取容器的类型（用于错误信息）
    raise TypeError(type(container))

# ============================================================================
# 函数：_accumulate_pair_stats（累积相邻层对的统计量）
# ============================================================================
def _accumulate_pair_stats(pair: Dict[str, any], a_probs: np.ndarray, a_inds: np.ndarray,
                           b_probs: np.ndarray, b_inds: np.ndarray, n_experts: int) -> int:
    """
    ========================================================================
    函数功能：累积相邻两层的统计量（Top-1 联合分布、Jaccard 相似度、稀疏余弦相似度）
    ========================================================================

    【在本项目中的应用】
    这是 Q1（层间相关性）的核心计算函数，用于累积：
    1. Top-1 联合频次矩阵（用于计算互信息和卡方统计量）
    2. Top-K Jaccard 相似度（集合重叠度）
    3. 稀疏余弦相似度（概率向量相似度）

    【参数】
    - pair: Dict，累积器字典，包含以下键：
      - "n_tokens": int，已处理的 token 数
      - "top1_joint": np.ndarray，Top-1 联合频次矩阵 (n_experts, n_experts)
      - "topk_jaccard_sum": float，Jaccard 相似度累积和
      - "sparse_cos_sum": float，稀疏余弦相似度累积和
    - a_probs: np.ndarray，第 i 层的概率矩阵，形状 (n_tokens, K)
    - a_inds: np.ndarray，第 i 层的专家索引矩阵，形状 (n_tokens, K)
    - b_probs: np.ndarray，第 i+1 层的概率矩阵，形状 (n_tokens, K)
    - b_inds: np.ndarray，第 i+1 层的专家索引矩阵，形状 (n_tokens, K)
    - n_experts: int，专家总数（通常是 60）

    【返回值】
    - int，实际处理的 token 数量
    """
    # ========================================================================
    # 步骤1：确定有效 token 数量
    # ========================================================================
    # 【为什么要取最小值？】
    # 因为两层的 token 数量可能不同（虽然理论上应该相同）
    # 取最小值确保不会越界访问
    #
    # a_probs.shape[0]：第 i 层的 token 数
    # b_probs.shape[0]：第 i+1 层的 token 数
    # min(...)：取两者的最小值
    L = min(a_probs.shape[0], b_probs.shape[0])

    # ========================================================================
    # 步骤2：边界检查
    # ========================================================================
    # 如果没有有效 token，直接返回 0
    if L <= 0:
        return 0

    # ========================================================================
    # 步骤3：更新 token 计数
    # ========================================================================
    # pair["n_tokens"] += L：累加处理的 token 数
    # 这个计数用于后续计算平均值
    pair["n_tokens"] += L

    # ========================================================================
    # 步骤4：提取 Top-1 专家 ID
    # ========================================================================
    # a_inds[:L, 0]：取第 i 层的前 L 个 token 的第 0 列（Top-1 专家）
    #   - :L：行切片，取前 L 行
    #   - 0：列索引，取第 0 列（Top-1）
    # .astype(np.int64, copy=False)：转换为 int64 类型
    #   - copy=False：如果已经是 int64，不复制（节省内存）
    top1_a = a_inds[:L, 0].astype(np.int64, copy=False)
    top1_b = b_inds[:L, 0].astype(np.int64, copy=False)

    # ========================================================================
    # 步骤5：过滤有效的专家 ID
    # ========================================================================
    # 【为什么要过滤？】
    # 因为数据中可能有无效的专家 ID（如 -1 或超出范围的值）
    #
    # (top1_a >= 0)：检查第 i 层的 Top-1 ID 是否 >= 0
    # (top1_a < n_experts)：检查第 i 层的 Top-1 ID 是否 < 60
    # &：逐元素逻辑与（numpy 的按位与运算符）
    # valid：布尔数组，形状 (L,)，True 表示该 token 的 Top-1 ID 有效
    valid = (top1_a >= 0) & (top1_a < n_experts) & (top1_b >= 0) & (top1_b < n_experts)

    # ========================================================================
    # 步骤6：累积 Top-1 联合频次矩阵
    # ========================================================================
    # 【什么是联合频次矩阵？】
    # joint[i, j] = "第 i 层选专家 i，第 i+1 层选专家 j" 的次数
    #
    # np.add.at(array, indices, values)：在指定位置累加值
    #   - array：目标数组（pair["top1_joint"]）
    #   - indices：索引元组 (top1_a[valid], top1_b[valid])
    #     - top1_a[valid]：第 i 层的有效 Top-1 ID
    #     - top1_b[valid]：第 i+1 层的有效 Top-1 ID
    #   - values：累加的值（这里是 1，表示出现 1 次）
    #
    # 【示例】
    # top1_a[valid] = [5, 12, 5]
    # top1_b[valid] = [3, 3, 8]
    # 执行后：joint[5, 3] += 1, joint[12, 3] += 1, joint[5, 8] += 1
    np.add.at(pair["top1_joint"], (top1_a[valid], top1_b[valid]), 1)

    # ========================================================================
    # 步骤7：逐 token 计算 Jaccard 相似度和稀疏余弦相似度
    # ========================================================================
    # 【为什么要逐 token 计算？】
    # 因为 Jaccard 和余弦相似度是"集合/向量级别"的度量
    # 需要对每个 token 单独计算，然后累加求平均
    #
    # for t in range(L)：遍历每个 token（从 0 到 L-1）
    for t in range(L):
        # ================================================================
        # 7.1 构建 Top-K 专家集合
        # ================================================================
        # 【什么是集合推导式？】
        # set(表达式 for 变量 in 可迭代对象 if 条件)
        # 这是 Python 的简洁语法，用于从列表生成集合
        #
        # 【逐步拆解】
        # a_inds[t]：第 t 个 token 在第 i 层的 Top-K 专家 ID 数组
        # for x in a_inds[t]：遍历这个数组的每个元素
        # if 0 <= int(x) < n_experts：过滤条件，只保留有效的专家 ID
        # int(x)：转换为整数（因为 x 可能是 numpy 类型）
        # set(...)：把结果放入集合（自动去重）
        #
        # 【示例】
        # a_inds[t] = [5, 12, 3, 45, -1, 0]
        # set_a = {5, 12, 3, 45, 0}  # -1 被过滤掉
        set_a = set(int(x) for x in a_inds[t] if 0 <= int(x) < n_experts)
        set_b = set(int(x) for x in b_inds[t] if 0 <= int(x) < n_experts)

        # ================================================================
        # 7.2 跳过空集合
        # ================================================================
        # 【为什么要跳过？】
        # 如果两个集合都为空，无法计算相似度，直接跳过
        #
        # not set_a：检查 set_a 是否为空（空集合在布尔上下文中为 False）
        # and：逻辑与
        # continue：跳过本次循环，进入下一次迭代
        if not set_a and not set_b:
            continue

        # ================================================================
        # 7.3 计算集合的交集和并集
        # ================================================================
        # 【集合运算】
        # set_a & set_b：交集（同时在两个集合中的元素）
        # set_a | set_b：并集（在任一集合中的元素）
        #
        # 【示例】
        # set_a = {5, 12, 3}
        # set_b = {12, 3, 8}
        # inter = {12, 3}  # 交集
        # union = {5, 12, 3, 8}  # 并集
        inter = set_a & set_b
        union = set_a | set_b

        # ================================================================
        # 7.4 计算 Jaccard 相似度
        # ================================================================
        # 【什么是 Jaccard 相似度？】
        # Jaccard(A, B) = |A ∩ B| / |A ∪ B|
        # 范围：[0, 1]
        # - 0：完全不重叠
        # - 1：完全相同
        #
        # 【计算步骤】
        # len(inter)：交集的大小
        # len(union)：并集的大小
        # len(inter) / len(union)：Jaccard 相似度
        # pair["topk_jaccard_sum"] += ...：累加到总和
        #
        # 【示例】
        # inter = {12, 3}，len(inter) = 2
        # union = {5, 12, 3, 8}，len(union) = 4
        # Jaccard = 2 / 4 = 0.5
        if union:
            pair["topk_jaccard_sum"] += len(inter) / len(union)

        # ================================================================
        # 7.5 计算稀疏余弦相似度（仅当有交集时）
        # ================================================================
        # 【什么是稀疏余弦相似度？】
        # 余弦相似度衡量两个向量的"方向相似性"
        # cos(A, B) = (A · B) / (||A|| * ||B||)
        # 其中：
        # - A · B：向量点积（内积）
        # - ||A||：向量 A 的模长（L2 范数）
        #
        # 【为什么叫"稀疏"？】
        # 因为我们只考虑 Top-K 专家（大部分专家概率为 0）
        # 只在"交集专家"上计算点积，其他位置视为 0
        #
        # 【为什么要检查 inter？】
        # 如果没有交集，点积必然为 0，余弦相似度为 0，可以跳过计算
        if inter:
            # ============================================================
            # 7.5.1 初始化累加器
            # ============================================================
            # dot：点积累加器（A · B）
            dot = 0.0
            # na：第 i 层向量的模长平方（||A||²）
            na = 0.0
            # nb：第 i+1 层向量的模长平方（||B||²）
            nb = 0.0

            # ============================================================
            # 7.5.2 计算第 i 层向量的模长平方
            # ============================================================
            # 【公式】||A||² = Σ(p_i²)
            # 遍历第 i 层的 Top-K 专家
            # a_inds.shape[1]：Top-K 的 K 值（通常是 6 或 8）
            for k in range(a_inds.shape[1]):
                # eid：专家 ID
                eid = int(a_inds[t, k])
                # pv：该专家的概率值
                pv = float(a_probs[t, k])
                # 只累加有效专家的贡献
                if 0 <= eid < n_experts:
                    # na += pv²：累加概率的平方
                    na += pv * pv

            # ============================================================
            # 7.5.3 计算第 i+1 层向量的模长平方
            # ============================================================
            # 【公式】||B||² = Σ(p_j²)
            # 遍历第 i+1 层的 Top-K 专家
            for k in range(b_inds.shape[1]):
                eid = int(b_inds[t, k])
                pv = float(b_probs[t, k])
                if 0 <= eid < n_experts:
                    nb += pv * pv

            # ============================================================
            # 7.5.4 构建专家 ID → 概率的映射字典
            # ============================================================
            # 【为什么需要字典？】
            # 因为后面要在"交集专家"上计算点积
            # 字典可以快速查找某个专家的概率值
            #
            # 【字典推导式】
            # {键表达式: 值表达式 for 变量 in 可迭代对象 if 条件}
            #
            # amap[eid] = prob：第 i 层专家 eid 的概率
            # 例如：{5: 0.3, 12: 0.2, 3: 0.15, ...}
            amap = {int(a_inds[t, k]): float(a_probs[t, k]) for k in range(a_inds.shape[1]) if 0 <= int(a_inds[t, k]) < n_experts}
            bmap = {int(b_inds[t, k]): float(b_probs[t, k]) for k in range(b_inds.shape[1]) if 0 <= int(b_inds[t, k]) < n_experts}

            # ============================================================
            # 7.5.5 计算点积（只在交集专家上）
            # ============================================================
            # 【公式】A · B = Σ(a_i * b_i)，只对交集专家求和
            # for e in inter：遍历交集中的每个专家 ID
            for e in inter:
                # amap.get(e, 0.0)：从字典中获取专家 e 的概率
                #   - 如果 e 不在字典中，返回默认值 0.0
                # bmap.get(e, 0.0)：同理
                # 两者相乘，累加到 dot
                dot += amap.get(e, 0.0) * bmap.get(e, 0.0)

            # ============================================================
            # 7.5.6 计算余弦相似度
            # ============================================================
            # 【公式】cos(A, B) = (A · B) / (||A|| * ||B||)
            #                   = dot / sqrt(na * nb)
            #
            # (na ** 0.5)：计算 sqrt(na)，即 ||A||
            # (nb ** 0.5)：计算 sqrt(nb)，即 ||B||
            # denom：分母 = ||A|| * ||B||
            denom = (na ** 0.5) * (nb ** 0.5)

            # 边界检查：避免除以 0
            if denom > 0:
                # dot / denom：余弦相似度
                # pair["sparse_cos_sum"] += ...：累加到总和
                pair["sparse_cos_sum"] += dot / denom

    # ========================================================================
    # 步骤8：返回处理的 token 数量
    # ========================================================================
    return L


# ============================================================================
# 函数：analyze_router_structure（分析路由结构 - Q1 核心函数）
# ============================================================================
def analyze_router_structure(
    all_data: Dict[str, any],
    output_dir: str,
    n_experts: int = 64,
    max_samples: int = -1,
    baseline: bool = False,
    baseline_seed: int = 42,
    baseline_permutations: int = 5,
    baseline_max_tokens: int = -1,
    max_layer_distance: int = 1,
):
    """
    ========================================================================
    函数功能：分析 MoE 路由结构的层间相关性（Q1 的核心分析函数）
    ========================================================================

    【这个函数做什么？】
    这是整个 Q1 分析的核心函数，用于计算：
    1. 每层的统计量（Top-1 熵、负载 Gini、自转移率、run length 等）
    2. 相邻层对的相关性（互信息、卡方检验、Jaccard、余弦相似度）
    3. 置乱基线（用于显著性检验）

    【参数】
    - all_data: Dict，load_data 返回的数据字典
    - output_dir: str，输出目录路径
    - n_experts: int，专家总数（默认 64，实际是 60）
    - max_samples: int，最多处理多少个样本（-1 表示全部）
    - baseline: bool，是否计算置乱基线
    - baseline_seed: int，置乱基线的随机种子
    - baseline_permutations: int，置乱次数
    - baseline_max_tokens: int，置乱基线最多使用多少 token
    - max_layer_distance: int，最大层间距离（1 表示只看相邻层）

    【返回值】
    - Dict，包含所有统计结果的字典
    """
    # ========================================================================
    # 步骤1：创建输出目录
    # ========================================================================
    # os.makedirs：创建目录（包括父目录）
    # exist_ok=True：如果目录已存在，不报错
    os.makedirs(output_dir, exist_ok=True)

    # ========================================================================
    # 步骤2：提取并排序层名
    # ========================================================================
    # _sorted_layer_names：调用前面定义的函数，按层序号排序
    # 例如：["MoE_Gate0", "MoE_Gate1", "MoE_Gate2", ...]
    layer_names = _sorted_layer_names(all_data)

    # 边界检查：如果没有找到任何层数据，抛出异常
    if not layer_names:
        raise ValueError("未找到任何 *_probs 层数据")

    # ========================================================================
    # 步骤3：确定样本数量
    # ========================================================================
    # all_data.get("tokens_str", [])：获取 token 文本列表
    #   - 如果键不存在，返回空列表 []
    tokens_str = all_data.get("tokens_str", [])

    # len(tokens_str)：样本总数
    n_samples = len(tokens_str)

    # 如果指定了 max_samples，则限制样本数量
    if max_samples > 0:
        n_samples = min(n_samples, max_samples)

    # ========================================================================
    # 步骤4：推断 Top-K 的 K 值
    # ========================================================================
    # 【为什么要推断 K？】
    # 因为不同数据集的 Top-K 可能不同（通常是 6 或 8）
    # 我们从第一个样本的第一层数据中推断
    #
    # _get_sample_item：从容器中提取第 0 个样本
    # f"{layer_names[0]}_probs"：第一层的概率数据键名
    first_probs = _get_sample_item(all_data[f"{layer_names[0]}_probs"], 0)

    # first_probs.shape[1]：概率矩阵的列数，即 K 值
    # isinstance(..., np.ndarray)：检查是否是 numpy 数组
    # first_probs.ndim == 2：检查是否是 2D 数组
    # 如果条件满足，取列数；否则返回 0
    topk = int(first_probs.shape[1]) if isinstance(first_probs, np.ndarray) and first_probs.ndim == 2 else 0

    # ========================================================================
    # 步骤5：初始化数据结构（用于累积统计量）
    # ========================================================================
    # 【这些数据结构的作用】
    # 在遍历所有样本时，我们需要累积各种统计量
    # 这里预先创建好所有需要的容器

    # per_layer：存储每层的最终统计结果（字典）
    per_layer = {}

    # layer_top1_counts：每层的 Top-1 专家频次
    # {层名: 频次数组(60,)}
    # 例如：layer_top1_counts["MoE_Gate0"][5] = 专家 5 被选为 Top-1 的次数
    layer_top1_counts = {ln: np.zeros(n_experts, dtype=np.int64) for ln in layer_names}

    # layer_transitions：每层的转移矩阵（自转移）
    # {层名: 转移矩阵(60, 60)}
    # transitions[i, j] = "前一个 token 选专家 i，当前 token 选专家 j" 的次数
    layer_transitions = {ln: np.zeros((n_experts, n_experts), dtype=np.int64) for ln in layer_names}
    # layer_run_lengths：每层的连续段长度列表
    # {层名: [长度1, 长度2, ...]}
    # 用于分析专家切换频率
    layer_run_lengths = {ln: [] for ln in layer_names}

    # layer_load：每层的专家负载（概率累加）
    # {层名: 负载数组(60,)}
    # load[i] = 专家 i 的总概率质量
    layer_load = {ln: np.zeros(n_experts, dtype=np.float64) for ln in layer_names}

    # layer_token_count：每层处理的 token 总数
    # {层名: token数}
    layer_token_count = {ln: 0 for ln in layer_names}

    # ========================================================================
    # 步骤6：初始化相邻层对的数据结构
    # ========================================================================
    # 【什么是相邻层对？】
    # 相邻层对指的是 (第 i 层, 第 i+1 层) 的组合
    # 例如：(MoE_Gate0, MoE_Gate1), (MoE_Gate1, MoE_Gate2), ...
    #
    # 【字典推导式】
    # {键表达式: 值表达式 for 变量 in 可迭代对象}
    #
    # f"{layer_names[i]}->{layer_names[i+1]}"：层对的键名
    # 例如："MoE_Gate0->MoE_Gate1"
    #
    # range(len(layer_names) - 1)：遍历 0 到 n-2
    # 因为最后一层没有"下一层"，所以要减 1
    adjacent_pairs = {f"{layer_names[i]}->{layer_names[i+1]}": {
        "n_tokens": 0,                    # 处理的 token 数
        "topk_jaccard_sum": 0.0,          # Jaccard 相似度累积和
        "sparse_cos_sum": 0.0,            # 稀疏余弦相似度累积和
        "top1_joint": np.zeros((n_experts, n_experts), dtype=np.int64),  # Top-1 联合频次矩阵
    } for i in range(len(layer_names) - 1)}
    # ========================================================================
    # 步骤7：如果启用置乱基线，添加额外的数据容器
    # ========================================================================
    # 【什么是置乱基线？】
    # 置乱基线用于显著性检验：
    # - 把第 i+1 层的数据随机打乱（permutation）
    # - 重新计算互信息等统计量
    # - 如果真实值显著高于置乱值，说明相关性是真实的
    #
    # baseline：布尔值，是否启用置乱基线
    if baseline:
        # 遍历每个相邻层对，添加额外的数据容器
        for pair in adjacent_pairs.values():
            # 这些列表用于存储原始数据，供后续置乱使用
            pair["top1_a"] = []           # 第 i 层的 Top-1 ID 列表
            pair["top1_b"] = []           # 第 i+1 层的 Top-1 ID 列表
            pair["topk_a_inds"] = []      # 第 i 层的 Top-K 索引列表
            pair["topk_a_probs"] = []     # 第 i 层的 Top-K 概率列表
            pair["topk_b_inds"] = []      # 第 i+1 层的 Top-K 索引列表
            pair["topk_b_probs"] = []     # 第 i+1 层的 Top-K 概率列表

    # ========================================================================
    # 步骤8：初始化层间距离对的数据结构（可选）
    # ========================================================================
    # 【什么是层间距离对？】
    # 除了相邻层（距离=1），我们还可以分析更远的层对
    # 例如：距离=2 表示 (第 i 层, 第 i+2 层)
    #
    # max_layer_distance：最大层间距离
    # 如果 > 1，则需要初始化额外的数据结构
    distance_pairs = None
    if max_layer_distance > 1:
        # 字典推导式：为每个距离创建数据容器
        # d：距离值，从 1 到 max_layer_distance
        distance_pairs = {d: {
            "n_tokens": 0,
            "topk_jaccard_sum": 0.0,
            "sparse_cos_sum": 0.0,
            "top1_joint": np.zeros((n_experts, n_experts), dtype=np.int64),
        } for d in range(1, max_layer_distance + 1)}

    # ========================================================================
    # 步骤9：主循环 - 遍历所有样本
    # ========================================================================
    # 【这个循环做什么？】
    # 遍历每个样本（每个样本是一个序列），累积各种统计量
    #
    # for s in range(n_samples)：遍历样本索引（从 0 到 n_samples-1）
    for s in range(n_samples):
        # ================================================================
        # 9.1 为当前样本创建临时容器
        # ================================================================
        # 【为什么需要临时容器？】
        # 因为每个样本可能包含多层数据
        # 我们先把当前样本的所有层数据提取出来，放在字典里
        # 这样后续处理时可以方便地访问任意层的数据
        #
        # layer_probs_sample：{层名: 概率矩阵}
        layer_probs_sample = {}
        # layer_inds_sample：{层名: 索引矩阵}
        layer_inds_sample = {}
        # ================================================================
        # 9.2 提取当前样本的所有层数据
        # ================================================================
        # 遍历每一层，提取当前样本的数据
        for ln in layer_names:
            # _get_sample_item：从容器中提取第 s 个样本
            # f"{ln}_probs"：层名对应的概率数据键
            p = _get_sample_item(all_data[f"{ln}_probs"], s)
            # f"{ln}_indices"：层名对应的索引数据键
            i = _get_sample_item(all_data[f"{ln}_indices"], s)

            # 数据有效性检查
            # isinstance(..., np.ndarray)：检查是否是 numpy 数组
            if not isinstance(p, np.ndarray) or not isinstance(i, np.ndarray):
                # 如果不是数组，跳过这一层
                continue

            # 维度检查：必须是 2D 数组
            # p.ndim：数组的维度数
            if p.ndim != 2 or i.ndim != 2:
                # 如果不是 2D，跳过这一层
                continue

            # 数据有效，保存到临时容器
            layer_probs_sample[ln] = p
            layer_inds_sample[ln] = i

        # ================================================================
        # 9.3 遍历每一层，累积层内统计量
        # ================================================================
        # 【这个循环做什么？】
        # 对当前样本的每一层，累积以下统计量：
        # 1. Top-1 专家频次
        # 2. 转移矩阵（自转移）
        # 3. Run length（连续段长度）
        # 4. 专家负载（概率累加）
        for ln in layer_names:
            # 检查当前层是否有有效数据
            if ln not in layer_probs_sample:
                continue

            # 提取当前层的数据
            probs = layer_probs_sample[ln]
            inds = layer_inds_sample[ln]

            # seq_len：当前样本在这一层的 token 数量
            seq_len = int(probs.shape[0])

            # 累加 token 计数
            layer_token_count[ln] += seq_len

            # ============================================================
            # 9.3.1 累积 Top-1 专家频次
            # ============================================================
            # inds[:, 0]：提取第 0 列（Top-1 专家 ID）
            # .astype(np.int64, copy=False)：转换为 int64 类型
            top1 = inds[:, 0].astype(np.int64, copy=False)

            # 过滤有效的专家 ID
            # valid_top1：布尔数组，标记哪些 token 的 Top-1 ID 有效
            valid_top1 = (top1 >= 0) & (top1 < n_experts)

            # 累加频次
            # np.add.at：在指定位置累加值
            # layer_top1_counts[ln]：当前层的频次数组
            # top1[valid_top1]：有效的 Top-1 ID
            # 1：每次累加 1
            np.add.at(layer_top1_counts[ln], top1[valid_top1], 1)

            # ============================================================
            # 9.3.2 累积转移矩阵（自转移）
            # ============================================================
            # 【什么是转移矩阵？】
            # transitions[i, j] = "前一个 token 选专家 i，当前 token 选专家 j" 的次数
            # 用于分析"专家切换模式"
            #
            # 只有序列长度 >= 2 时才能计算转移
            if seq_len >= 2:
                # top1[:-1]：前 n-1 个 token 的 Top-1 ID（作为"前状态"）
                # top1[1:]：后 n-1 个 token 的 Top-1 ID（作为"后状态"）
                a = top1[:-1]
                b = top1[1:]

                # 过滤有效的转移对
                valid = (a >= 0) & (a < n_experts) & (b >= 0) & (b < n_experts)
                a = a[valid]
                b = b[valid]

                # 累加转移频次
                # (a, b)：索引元组，表示从专家 a 转移到专家 b
                np.add.at(layer_transitions[ln], (a, b), 1)

            # ============================================================
            # 9.3.3 累积 run length（连续段长度）
            # ============================================================
            # _run_lengths：计算连续相同值的段长度
            # top1[valid_top1]：只对有效的 Top-1 ID 计算
            # .extend(...)：把新的 run length 列表追加到总列表
            layer_run_lengths[ln].extend(_run_lengths(top1[valid_top1]))

            # ============================================================
            # 9.3.4 累积专家负载（概率累加）
            # ============================================================
            # 遍历 Top-K 的每一列（每个 k 值）
            for k in range(inds.shape[1]):
                # eid：当前列的所有专家 ID（形状：(seq_len,)）
                eid = inds[:, k].astype(np.int64, copy=False)
                # pv：当前列的所有概率值（形状：(seq_len,)）
                pv = probs[:, k].astype(np.float64, copy=False)

                # 过滤有效的专家 ID
                valid = (eid >= 0) & (eid < n_experts)

                # 累加负载
                # layer_load[ln][eid[valid]]：对应专家的负载
                # pv[valid]：对应的概率值
                np.add.at(layer_load[ln], eid[valid], pv[valid])

        # ============================================================
        # 9.4 遍历所有相邻层对,累积层对统计量
        # ============================================================
        # 【目的】
        # 计算相邻层之间的相关性指标(互信息、卡方统计量、Jaccard 相似度等)
        # 这是 Q1 的核心:判断相邻层的专家选择是否相关

        # 遍历所有相邻层对(layer_i -> layer_{i+1})
        for i in range(len(layer_names) - 1):
            # a_ln: 前一层的名称(例如 "layers/MoE_Gate0")
            a_ln = layer_names[i]
            # b_ln: 后一层的名称(例如 "layers/MoE_Gate1")
            b_ln = layer_names[i + 1]

            # 检查当前样本是否包含这两层的数据
            # (某些样本可能缺失某些层)
            if a_ln not in layer_probs_sample or b_ln not in layer_probs_sample:
                continue  # 跳过缺失数据的层对

            # 提取前一层的概率和索引
            a_probs = layer_probs_sample[a_ln]  # 形状: (seq_len, top_k)
            a_inds = layer_inds_sample[a_ln]    # 形状: (seq_len, top_k)

            # 提取后一层的概率和索引
            b_probs = layer_probs_sample[b_ln]  # 形状: (seq_len, top_k)
            b_inds = layer_inds_sample[b_ln]    # 形状: (seq_len, top_k)

            # 构造层对的键名(例如 "layers/MoE_Gate0->layers/MoE_Gate1")
            pair_key = f"{a_ln}->{b_ln}"

            # 获取该层对的统计容器
            pair = adjacent_pairs[pair_key]

            # 调用 _accumulate_pair_stats 函数累积统计量
            # 返回值 L: 有效的序列长度(对齐后的最小长度)
            L = _accumulate_pair_stats(pair, a_probs, a_inds, b_probs, b_inds, n_experts)

            # --------------------------------------------------------
            # 9.4.1 如果需要计算置乱基线,保存原始数据
            # --------------------------------------------------------
            # 【置乱基线的作用】
            # 通过随机打乱层间对应关系,计算"无相关性"时的统计量
            # 用于判断真实相关性是否显著高于随机水平

            if baseline and L > 0:
                # 提取前一层的 Top-1 专家 ID
                # a_inds[:L, 0]: 取前 L 个 token 的第 0 列(Top-1)
                top1_a = a_inds[:L, 0].astype(np.int64, copy=False)

                # 提取后一层的 Top-1 专家 ID
                top1_b = b_inds[:L, 0].astype(np.int64, copy=False)

                # 过滤有效的专家 ID(范围在 [0, n_experts) 内)
                valid = (top1_a >= 0) & (top1_a < n_experts) & (top1_b >= 0) & (top1_b < n_experts)

                # 如果存在有效数据,保存到 baseline 容器
                if np.any(valid):
                    # 保存前一层的 Top-1 专家 ID(仅有效部分)
                    pair["top1_a"].append(top1_a[valid])
                    # 保存后一层的 Top-1 专家 ID(仅有效部分)
                    pair["top1_b"].append(top1_b[valid])

                    # 保存前一层的 Top-K 索引(用于计算 Jaccard 相似度)
                    pair["topk_a_inds"].append(a_inds[:L][valid])
                    # 保存前一层的 Top-K 概率(用于计算余弦相似度)
                    pair["topk_a_probs"].append(a_probs[:L][valid])

                    # 保存后一层的 Top-K 索引
                    pair["topk_b_inds"].append(b_inds[:L][valid])
                    # 保存后一层的 Top-K 概率
                    pair["topk_b_probs"].append(b_probs[:L][valid])

        # ============================================================
        # 9.5 如果需要,累积"距离层对"的统计量
        # ============================================================
        # 【距离层对的作用】
        # 除了相邻层(距离=1),还可以分析"隔层相关性"
        # 例如: layer_0 -> layer_2 (距离=2), layer_0 -> layer_3 (距离=3)
        # 用于研究"相关性是否随层间距离衰减"

        if distance_pairs is not None:
            # 遍历所有距离(从 1 到 max_layer_distance)
            for d in range(1, max_layer_distance + 1):
                # 遍历所有可能的层对(间隔为 d)
                for i in range(len(layer_names) - d):
                    # a_ln: 前一层(例如 layer_0)
                    a_ln = layer_names[i]
                    # b_ln: 后一层(例如 layer_2,如果 d=2)
                    b_ln = layer_names[i + d]

                    # 检查当前样本是否包含这两层的数据
                    if a_ln not in layer_probs_sample or b_ln not in layer_probs_sample:
                        continue  # 跳过缺失数据的层对

                    # 调用 _accumulate_pair_stats 累积统计量
                    # 注意: 这里不保存 baseline 数据(只关心真实相关性)
                    _accumulate_pair_stats(
                        distance_pairs[d],           # 距离为 d 的统计容器
                        layer_probs_sample[a_ln],    # 前一层的概率
                        layer_inds_sample[a_ln],     # 前一层的索引
                        layer_probs_sample[b_ln],    # 后一层的概率
                        layer_inds_sample[b_ln],     # 后一层的索引
                        n_experts,                   # 专家总数
                    )

    # ========================================================================
    # 步骤10：汇总每一层的统计量
    # ========================================================================
    # 【目的】
    # 将前面累积的原始数据(频次、转移矩阵、run length等)转换为最终指标
    # 包括: 熵、Gini系数、有效专家数、自转移率、run length统计等

    for ln in layer_names:
        # ----------------------------------------------------------------
        # 10.1 计算专家负载分布的指标
        # ----------------------------------------------------------------
        # 复制负载数组(避免修改原始数据)
        load = layer_load[ln].copy()
        # 计算总负载(所有专家的概率累加和)
        load_sum = float(load.sum())

        # 归一化为概率分布(和为1)
        if load_sum > 0:
            load /= load_sum  # 每个专家的负载占比

        # ----------------------------------------------------------------
        # 10.2 计算 Top-1 专家选择的熵
        # ----------------------------------------------------------------
        # 【熵的含义】
        # 衡量 Top-1 专家选择的"不确定性"或"均匀程度"
        # 熵越高,说明专家选择越分散;熵越低,说明集中在少数专家

        # 获取 Top-1 频次数组
        top1_counts = layer_top1_counts[ln].astype(np.float64)
        # 计算原始熵(自然对数)
        top1_entropy = _entropy(top1_counts)
        # 归一化熵: 除以最大可能熵 log(n_experts)
        # 归一化后的熵范围在 [0, 1],便于跨不同专家数的模型对比
        top1_entropy_norm = top1_entropy / np.log(n_experts) if n_experts > 1 else 0.0

        # ----------------------------------------------------------------
        # 10.3 计算转移矩阵的熵(衡量专家切换的不确定性)
        # ----------------------------------------------------------------
        # 【转移矩阵的含义】
        # trans[i, j]: 从专家 i 转移到专家 j 的次数
        # 转移概率矩阵: tprob[i, j] = trans[i, j] / sum(trans[i, :])

        # 获取转移矩阵
        trans = layer_transitions[ln].astype(np.float64)
        # 计算每一行的和(每个专家作为"前一个专家"的总次数)
        # keepdims=True: 保持维度为 (n_experts, 1),便于广播
        row_sums = trans.sum(axis=1, keepdims=True)

        # 计算转移概率矩阵
        # np.errstate: 临时忽略除零警告
        # where=row_sums > 0: 只对非零行进行除法,零行保持为0
        with np.errstate(divide="ignore", invalid="ignore"):
            tprob = np.divide(trans, row_sums, out=np.zeros_like(trans), where=row_sums > 0)

        # 计算每一行的熵(每个专家的"转移不确定性")
        row_entropy = []
        for r in range(n_experts):
            # 只计算有转移记录的专家
            if row_sums[r, 0] > 0:
                # tprob[r]: 第 r 个专家的转移概率分布
                row_entropy.append(_entropy(tprob[r]))

        # 计算所有专家的平均转移熵
        row_entropy_mean = float(np.mean(row_entropy)) if row_entropy else 0.0
        # 归一化(除以最大可能熵)
        row_entropy_norm = row_entropy_mean / np.log(n_experts) if n_experts > 1 else 0.0

        # ----------------------------------------------------------------
        # 10.4 计算自转移率(专家连续性指标)
        # ----------------------------------------------------------------
        # 【自转移率的含义】
        # 衡量"连续两个 token 选择同一个专家"的比例
        # 自转移率越高,说明专家选择越稳定(连续性强)

        # 计算总转移次数
        total_trans = float(trans.sum())
        # 计算自转移次数(对角线元素之和)
        # np.trace(trans): 矩阵的迹(对角线元素之和)
        self_trans = float(np.trace(trans)) / total_trans if total_trans > 0 else 0.0

        # ----------------------------------------------------------------
        # 10.5 计算 run length 统计量
        # ----------------------------------------------------------------
        # 【run length 的含义】
        # 连续选择同一个专家的 token 数量
        # 例如: [5,5,5,12,12] -> run lengths = [3, 2]

        # 获取该层的所有 run length
        runs = layer_run_lengths[ln]
        # 计算均值
        run_mean = float(np.mean(runs)) if runs else 0.0
        # 计算中位数(50分位数)
        run_p50 = float(np.percentile(runs, 50)) if runs else 0.0
        # 计算90分位数(衡量"长连续段"的典型长度)
        run_p90 = float(np.percentile(runs, 90)) if runs else 0.0

        # ----------------------------------------------------------------
        # 10.6 汇总该层的所有指标到字典
        # ----------------------------------------------------------------
        per_layer[ln] = {
            "n_tokens": int(layer_token_count[ln]),                      # token 总数
            "top1_entropy": float(top1_entropy),                         # Top-1 熵(原始)
            "top1_entropy_norm": float(top1_entropy_norm),               # Top-1 熵(归一化)
            "load_gini": float(_gini(load)),                             # 负载 Gini 系数
            "effective_n": float(_effective_n(load)),                    # 有效专家数
            "self_transition": float(self_trans),                        # 自转移率
            "transition_entropy_norm_mean": float(row_entropy_norm),     # 转移熵(归一化均值)
            "run_length_mean": float(run_mean),                          # run length 均值
            "run_length_p50": float(run_p50),                            # run length 中位数
            "run_length_p90": float(run_p90),                            # run length 90分位数
        }

    # ========================================================================
    # 辅助函数：_concat_list
    # ========================================================================
    # 【功能】
    # 将列表中的多个数组拼接成一个大数组
    # 如果列表为空,返回空数组
    #
    # 【参数】
    # - items: 数组列表,例如 [array1, array2, array3]
    # - axis: 拼接的轴(默认为0,即按行拼接)
    #
    # 【返回值】
    # - 拼接后的数组,如果输入为空则返回空数组
    #
    # 【示例】
    # items = [np.array([1,2]), np.array([3,4]), np.array([5,6])]
    # result = _concat_list(items)  # 返回 array([1,2,3,4,5,6])

    def _concat_list(items, axis=0):
        # 如果列表为空,返回空数组(形状为 (0,))
        if not items:
            return np.empty((0,), dtype=np.int64)
        # 使用 np.concatenate 拼接所有数组
        # axis=0: 按第0轴(行)拼接
        return np.concatenate(items, axis=axis)

    # ========================================================================
    # 辅助函数：_baseline_stats_for_pair
    # ========================================================================
    # 【功能】
    # 计算层对的"置乱基线"统计量,用于判断真实相关性是否显著
    #
    # 【置乱基线的原理】
    # 通过随机打乱层间对应关系,计算"无相关性"时的统计量分布
    # 如果真实统计量显著高于置乱基线,说明层间确实存在相关性
    #
    # 【参数】
    # - pair: 层对的统计容器,包含保存的原始数据(top1_a, top1_b等)
    # - observed_mi_norm: 观察到的归一化互信息(真实值)
    #
    # 【返回值】
    # - 字典,包含置乱基线的统计量(均值、标准差、p值等)
    # - 如果数据不足,返回 None
    #
    # 【两种置乱方式】
    # 1. permutation: 打乱 token 顺序(保持边缘分布不变)
    # 2. independence: 根据边缘分布独立采样(完全独立假设)

    def _baseline_stats_for_pair(pair: Dict[str, any], observed_mi_norm: float):
        # ----------------------------------------------------------------
        # 步骤1：拼接所有样本的数据
        # ----------------------------------------------------------------
        # 拼接前一层的 Top-1 专家 ID
        top1_a = _concat_list(pair.get("top1_a", []), axis=0)
        # 拼接后一层的 Top-1 专家 ID
        top1_b = _concat_list(pair.get("top1_b", []), axis=0)

        # 检查数据是否为空
        if top1_a.size == 0 or top1_b.size == 0:
            return None  # 数据不足,无法计算基线

        # 拼接 Top-K 索引和概率(用于计算 Jaccard 和余弦相似度)
        a_inds = _concat_list(pair.get("topk_a_inds", []), axis=0)
        a_probs = _concat_list(pair.get("topk_a_probs", []), axis=0)
        b_inds = _concat_list(pair.get("topk_b_inds", []), axis=0)
        b_probs = _concat_list(pair.get("topk_b_probs", []), axis=0)

        # ----------------------------------------------------------------
        # 步骤2：如果数据量太大,随机采样以加速计算
        # ----------------------------------------------------------------
        # 获取 token 总数
        n_tokens = top1_a.size
        # 创建随机数生成器(使用固定种子保证可复现)
        rng = np.random.default_rng(baseline_seed)

        # 如果 token 数超过最大限制,随机采样
        if baseline_max_tokens > 0 and n_tokens > baseline_max_tokens:
            # 随机选择 baseline_max_tokens 个索引(不放回采样)
            idx = rng.choice(n_tokens, size=baseline_max_tokens, replace=False)
            # 对所有数组进行采样
            top1_a = top1_a[idx]
            top1_b = top1_b[idx]
            a_inds = a_inds[idx]
            a_probs = a_probs[idx]
            b_inds = b_inds[idx]
            b_probs = b_probs[idx]
            # 更新 token 数
            n_tokens = top1_a.size

        # ----------------------------------------------------------------
        # 步骤3：置乱基线循环(permutation方式)
        # ----------------------------------------------------------------
        # 【置乱方式1：permutation】
        # 随机打乱后一层的 token 顺序,破坏层间对应关系
        # 但保持每一层的边缘分布不变(专家选择频率不变)

        # 初始化结果容器
        mi_perm = []   # 互信息(permutation)
        jac_perm = []  # Jaccard 相似度(permutation)
        cos_perm = []  # 余弦相似度(permutation)

        # 重复 baseline_permutations 次置乱实验
        for _ in range(max(1, baseline_permutations)):
            # 生成随机排列(打乱 token 顺序)
            # perm: 长度为 n_tokens 的随机排列,例如 [2,0,3,1,...]
            perm = rng.permutation(n_tokens)

            # --------------------------------------------------------
            # 3.1 计算置乱后的互信息
            # --------------------------------------------------------
            # 创建联合频次矩阵(n_experts × n_experts)
            joint = np.zeros((n_experts, n_experts), dtype=np.int64)
            # 累积联合频次: joint[top1_a[i], top1_b[perm[i]]] += 1
            # 注意: top1_b 被打乱了,所以层间对应关系被破坏
            np.add.at(joint, (top1_a, top1_b[perm]), 1)
            # 计算归一化互信息并保存
            mi_perm.append(_mi_from_joint(joint) / np.log(n_experts) if n_experts > 1 else 0.0)

            # --------------------------------------------------------
            # 3.2 计算置乱后的 Jaccard 相似度和余弦相似度
            # --------------------------------------------------------
            # 初始化累加器
            jac_sum = 0.0  # Jaccard 相似度累加
            cos_sum = 0.0  # 余弦相似度累加

            # 遍历每个 token
            for t in range(n_tokens):
                # 提取前一层的 Top-K 专家集合(第 t 个 token)
                # 使用集合推导式,过滤有效的专家 ID
                set_a = set(int(x) for x in a_inds[t] if 0 <= int(x) < n_experts)

                # 提取后一层的 Top-K 专家集合(第 perm[t] 个 token,已打乱)
                set_b = set(int(x) for x in b_inds[perm[t]] if 0 <= int(x) < n_experts)

                # 如果两个集合都为空,跳过
                if not set_a and not set_b:
                    continue

                # 计算交集和并集
                inter = set_a & set_b  # 交集(共同的专家)
                union = set_a | set_b  # 并集(所有专家)

                # 计算 Jaccard 相似度: |交集| / |并集|
                if union:
                    jac_sum += len(inter) / len(union)

                # 只有当存在交集时才计算余弦相似度
                if inter:
                    # 【余弦相似度计算】
                    # cos = dot(a, b) / (||a|| * ||b||)
                    # 其中 dot(a, b) 是点积, ||a|| 是向量 a 的模长

                    # 初始化
                    dot = 0.0  # 点积
                    na = 0.0   # ||a||^2 (前一层向量的模长平方)
                    nb = 0.0   # ||b||^2 (后一层向量的模长平方)

                    # 计算前一层向量的模长平方
                    for k in range(a_inds.shape[1]):
                        eid = int(a_inds[t, k])      # 专家 ID
                        pv = float(a_probs[t, k])    # 概率值
                        if 0 <= eid < n_experts:
                            na += pv * pv  # 累加 p^2

                    # 计算后一层向量的模长平方(注意使用 perm[t])
                    for k in range(b_inds.shape[1]):
                        eid = int(b_inds[perm[t], k])
                        pv = float(b_probs[perm[t], k])
                        if 0 <= eid < n_experts:
                            nb += pv * pv

                    # 构建专家ID到概率的映射(用于快速查找)
                    # amap: {专家ID: 概率值} for 前一层
                    amap = {int(a_inds[t, k]): float(a_probs[t, k]) for k in range(a_inds.shape[1]) if 0 <= int(a_inds[t, k]) < n_experts}
                    # bmap: {专家ID: 概率值} for 后一层
                    bmap = {int(b_inds[perm[t], k]): float(b_probs[perm[t], k]) for k in range(b_inds.shape[1]) if 0 <= int(b_inds[perm[t], k]) < n_experts}

                    # 计算点积(只对交集中的专家)
                    for e in inter:
                        dot += amap.get(e, 0.0) * bmap.get(e, 0.0)

                    # 计算余弦相似度: dot / (sqrt(na) * sqrt(nb))
                    denom = (na ** 0.5) * (nb ** 0.5)
                    if denom > 0:
                        cos_sum += dot / denom

            # 保存该次置乱的平均 Jaccard 和余弦相似度
            jac_perm.append(jac_sum / n_tokens if n_tokens > 0 else 0.0)
            cos_perm.append(cos_sum / n_tokens if n_tokens > 0 else 0.0)

        # ----------------------------------------------------------------
        # 步骤4：计算 p 值(permutation 基线)
        # ----------------------------------------------------------------
        # 【p 值的含义】
        # p 值表示"在零假设(无相关性)下,观察到当前或更极端结果的概率"
        # p 值越小,说明真实相关性越显著

        # 将 permutation 基线的互信息转换为数组
        p_vals = np.array(mi_perm, dtype=np.float64)
        # 计算 p 值: (基线中 >= 观察值的次数 + 1) / (总次数 + 1)
        # +1 是为了避免 p 值为 0(保守估计)
        p_val = float((np.sum(p_vals >= observed_mi_norm) + 1) / (len(p_vals) + 1)) if len(p_vals) > 0 else None

        # ----------------------------------------------------------------
        # 步骤5：独立性基线(independence方式)
        # ----------------------------------------------------------------
        # 【置乱方式2：independence】
        # 根据后一层的边缘分布独立采样,完全破坏层间依赖关系
        # 这是比 permutation 更强的零假设

        # 计算后一层的专家选择频次
        top1_counts = np.bincount(top1_b, minlength=n_experts).astype(np.float64)
        # 归一化为概率分布
        p = top1_counts / top1_counts.sum() if top1_counts.sum() > 0 else np.full(n_experts, 1.0 / n_experts)

        # 初始化独立性基线的互信息列表
        mi_ind = []
        # 重复 baseline_permutations 次独立采样实验
        for _ in range(max(1, baseline_permutations)):
            # 根据概率分布 p 独立采样后一层的专家 ID
            # replace=True: 有放回采样(允许重复)
            top1_b_ind = rng.choice(n_experts, size=n_tokens, replace=True, p=p)
            # 创建联合频次矩阵
            joint = np.zeros((n_experts, n_experts), dtype=np.int64)
            # 累积联合频次
            np.add.at(joint, (top1_a, top1_b_ind), 1)
            # 计算归一化互信息并保存
            mi_ind.append(_mi_from_joint(joint) / np.log(n_experts) if n_experts > 1 else 0.0)

        # ----------------------------------------------------------------
        # 步骤6：返回置乱基线的统计结果
        # ----------------------------------------------------------------
        return {
            # permutation 基线的统计量
            "permutation": {
                "top1_mi_norm_mean": float(np.mean(mi_perm)) if mi_perm else 0.0,      # 互信息均值
                "top1_mi_norm_p5": float(np.percentile(mi_perm, 5)) if mi_perm else 0.0,   # 5分位数
                "top1_mi_norm_p95": float(np.percentile(mi_perm, 95)) if mi_perm else 0.0, # 95分位数
                "topk_jaccard_mean": float(np.mean(jac_perm)) if jac_perm else 0.0,    # Jaccard均值
                "sparse_cos_mean": float(np.mean(cos_perm)) if cos_perm else 0.0,      # 余弦相似度均值
                "top1_mi_norm_p_value": p_val,                                          # p值
                "n_tokens": int(n_tokens),                                              # token数
            },
            # independence 基线的统计量
            "independent": {
                "top1_mi_norm_mean": float(np.mean(mi_ind)) if mi_ind else 0.0,        # 互信息均值
                "top1_mi_norm_p5": float(np.percentile(mi_ind, 5)) if mi_ind else 0.0,     # 5分位数
                "top1_mi_norm_p95": float(np.percentile(mi_ind, 95)) if mi_ind else 0.0,   # 95分位数
                "n_tokens": int(n_tokens),                                              # token数
            },
        }

    # ========================================================================
    # 步骤11：汇总所有相邻层对的统计量
    # ========================================================================
    # 【目的】
    # 将前面累积的原始数据(联合频次矩阵、Jaccard和、余弦和)转换为最终指标
    # 包括: 互信息、卡方统计量、Cramér's V、Jaccard相似度、余弦相似度等

    pair_out = {}  # 输出字典: {层对名称: 统计量字典}

    # 遍历所有相邻层对
    for k, pair in adjacent_pairs.items():
        # ----------------------------------------------------------------
        # 11.1 提取基本数据
        # ----------------------------------------------------------------
        # 获取 token 总数
        n = int(pair["n_tokens"])

        # 计算平均 Jaccard 相似度
        # pair["topk_jaccard_sum"]: 所有 token 的 Jaccard 相似度之和
        jac = float(pair["topk_jaccard_sum"]) / n if n > 0 else 0.0

        # 计算平均余弦相似度
        # pair["sparse_cos_sum"]: 所有 token 的余弦相似度之和
        cos = float(pair["sparse_cos_sum"]) / n if n > 0 else 0.0

        # ----------------------------------------------------------------
        # 11.2 计算互信息(Mutual Information)
        # ----------------------------------------------------------------
        # 获取联合频次矩阵(n_experts × n_experts)
        joint = pair["top1_joint"].astype(np.float64)
        # 计算原始互信息
        mi = _mi_from_joint(joint)
        # 归一化互信息: 除以 log(n_experts)
        mi_norm = float(mi) / np.log(n_experts) if n_experts > 1 else 0.0

        # ----------------------------------------------------------------
        # 11.3 计算卡方统计量(Chi-Square Test)
        # ----------------------------------------------------------------
        # 卡方检验用于判断两个分类变量是否独立
        chi2, p_val, df = _chi2_from_joint(joint)

        # 计算 Cramér's V 系数(归一化的卡方统计量)
        # Cramér's V 范围在 [0, 1],不受样本量影响
        cramers_v = _cramers_v(chi2, float(joint.sum()), joint.shape[0], joint.shape[1])

        # ----------------------------------------------------------------
        # 11.4 汇总该层对的所有指标
        # ----------------------------------------------------------------
        pair_out[k] = {
            "n_tokens": n,                          # token 总数
            "topk_jaccard_mean": jac,               # 平均 Jaccard 相似度
            "sparse_cos_mean": cos,                 # 平均余弦相似度
            "top1_mi_norm": mi_norm,                # 归一化互信息
            "top1_chi2": float(chi2),               # 卡方统计量
            "top1_chi2_p": p_val,                   # 卡方检验的 p 值
            "top1_chi2_df": int(df),                # 自由度
            "top1_cramers_v": float(cramers_v),     # Cramér's V 系数
        }
        # ----------------------------------------------------------------
        # 11.5 如果需要,添加置乱基线统计量
        # ----------------------------------------------------------------
        if baseline:
            # 调用 _baseline_stats_for_pair 计算置乱基线
            # 返回值包含 permutation 和 independent 两种基线的统计量
            pair_out[k]["baseline"] = _baseline_stats_for_pair(pair, mi_norm)

    # ========================================================================
    # 步骤12：汇总"距离层对"的统计量(如果需要)
    # ========================================================================
    # 【目的】
    # 分析"隔层相关性"(例如 layer_0 -> layer_2)
    # 用于研究相关性是否随层间距离衰减

    distance_out = {}  # 输出字典: {距离: 统计量字典}
    if distance_pairs is not None:
        # 遍历所有距离(1, 2, 3, ...)
        for d, pair in distance_pairs.items():
            # 提取基本数据
            n = int(pair["n_tokens"])
            jac = float(pair["topk_jaccard_sum"]) / n if n > 0 else 0.0
            cos = float(pair["sparse_cos_sum"]) / n if n > 0 else 0.0

            # 计算互信息
            joint = pair["top1_joint"].astype(np.float64)
            mi = _mi_from_joint(joint)
            mi_norm = float(mi) / np.log(n_experts) if n_experts > 1 else 0.0

            # 保存该距离的统计量
            distance_out[str(d)] = {
                "n_tokens": n,                      # token 总数
                "topk_jaccard_mean": jac,           # 平均 Jaccard 相似度
                "sparse_cos_mean": cos,             # 平均余弦相似度
                "top1_mi_norm": mi_norm,            # 归一化互信息
            }

    # ========================================================================
    # 步骤13：构建输出字典并保存到 JSON 文件
    # ========================================================================
    # 【输出结构】
    # {
    #   "meta": {...},                      # 元数据(专家数、层数、样本数等)
    #   "per_layer": {...},                 # 每一层的统计量
    #   "adjacent_layer_pairs": {...},      # 相邻层对的统计量
    #   "layer_distance_pairs": {...}       # 距离层对的统计量(可选)
    # }

    out = {
        # 元数据
        "meta": {
            "n_experts": int(n_experts),                        # 专家总数
            "n_layers": int(len(layer_names)),                  # 层数
            "n_samples": int(n_samples),                        # 样本数
            "topk": int(topk),                                  # Top-K 值
            "layer_names": layer_names,                         # 层名称列表
            "baseline_enabled": bool(baseline),                 # 是否启用置乱基线
            "baseline_seed": int(baseline_seed),                # 置乱基线的随机种子
            "baseline_permutations": int(baseline_permutations),# 置乱次数
            "baseline_max_tokens": int(baseline_max_tokens),    # 置乱基线的最大 token 数
            "max_layer_distance": int(max_layer_distance),      # 最大层间距离
        },
        # 每一层的统计量
        "per_layer": per_layer,
        # 相邻层对的统计量
        "adjacent_layer_pairs": pair_out,
    }

    # 如果有距离层对的统计量,添加到输出
    if distance_out:
        out["layer_distance_pairs"] = distance_out

    # 保存到 JSON 文件
    out_path = os.path.join(output_dir, "router_structure_stats.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)

    # 打印日志
    logging.info(f"Wrote router structure stats -> {out_path}")

    # 返回输出字典
    return out


# ============================================================================
# 主函数：main
# ============================================================================
# 【功能】
# 脚本的入口函数,负责:
# 1. 解析命令行参数
# 2. 加载数据
# 3. 根据模式调用相应的分析函数
#
# 【支持的模式】
# - router_stats: 路由结构分析(Q1的核心功能,分析层间相关性)
# - expert_load: 专家负载分析(分析专家使用频率和负载分布)

def main():
    # ========================================================================
    # 步骤1：创建命令行参数解析器
    # ========================================================================
    parser = argparse.ArgumentParser()

    # ----------------------------------------------------------------
    # 1.1 基本参数
    # ----------------------------------------------------------------
    # --mode: 分析模式
    # 可选值: "router_stats"(路由结构分析) 或 "expert_load"(专家负载分析)
    # 默认值: "router_stats"
    parser.add_argument("--mode", type=str, default="router_stats", choices=["router_stats", "expert_load"])

    # --data_dir: 数据目录(必需参数)
    # 包含 NPZ 文件的目录路径
    parser.add_argument("--data_dir", type=str, required=True)

    # --output_dir: 输出目录(必需参数)
    # 分析结果的保存路径
    parser.add_argument("--output_dir", type=str, required=True)

    # --max_files: 最多加载多少个 NPZ 文件
    # -1 表示加载所有文件
    parser.add_argument("--max_files", type=int, default=-1)

    # --n_experts: 专家总数
    # 默认值: 64(需要根据实际模型调整)
    parser.add_argument("--n_experts", type=int, default=64)

    # --max_samples: 最多处理多少个样本
    # -1 表示处理所有样本
    parser.add_argument("--max_samples", type=int, default=-1)

    # ----------------------------------------------------------------
    # 1.2 置乱基线相关参数
    # ----------------------------------------------------------------
    # --baseline: 是否计算置乱基线
    # action="store_true": 如果命令行中出现 --baseline,则设为 True
    parser.add_argument("--baseline", action="store_true")

    # --baseline_seed: 置乱基线的随机种子
    # 用于保证可复现性
    parser.add_argument("--baseline_seed", type=int, default=42)

    # --baseline_permutations: 置乱次数
    # 每个层对重复置乱多少次
    parser.add_argument("--baseline_permutations", type=int, default=5)

    # --baseline_max_tokens: 置乱基线的最大 token 数
    # -1 表示不限制(如果数据量太大,可以设置限制以加速计算)
    parser.add_argument("--baseline_max_tokens", type=int, default=-1)

    # ----------------------------------------------------------------
    # 1.3 距离层对相关参数
    # ----------------------------------------------------------------
    # --max_layer_distance: 最大层间距离
    # 1 表示只分析相邻层(layer_i -> layer_{i+1})
    # 2 表示分析相邻层和隔一层(layer_i -> layer_{i+1}, layer_i -> layer_{i+2})
    parser.add_argument("--max_layer_distance", type=int, default=1)

    # 解析命令行参数
    args = parser.parse_args()

    # ========================================================================
    # 步骤2：加载数据
    # ========================================================================
    # 调用 load_data 函数加载所有 NPZ 文件
    # 返回值: 字典,键为 "layer_name_probs" 和 "layer_name_indices"
    all_data = load_data(args.data_dir, max_files=args.max_files)

    # ========================================================================
    # 步骤3：根据模式调用相应的分析函数
    # ========================================================================
    if args.mode == "expert_load":
        # 模式1: 专家负载分析
        # 分析每个专家的使用频率和负载分布
        analyze_expert_load(all_data, args.output_dir)
        return  # 分析完成,退出

    # 模式2: 路由结构分析(Q1的核心功能)
    # 分析层间相关性(互信息、卡方统计量、Jaccard相似度等)
    analyze_router_structure(
        all_data,                                   # 加载的数据
        args.output_dir,                            # 输出目录
        n_experts=args.n_experts,                   # 专家总数
        max_samples=args.max_samples,               # 最大样本数
        baseline=args.baseline,                     # 是否计算置乱基线
        baseline_seed=args.baseline_seed,           # 置乱基线的随机种子
        baseline_permutations=args.baseline_permutations,  # 置乱次数
        baseline_max_tokens=args.baseline_max_tokens,      # 置乱基线的最大 token 数
        max_layer_distance=args.max_layer_distance,        # 最大层间距离
    )


# ============================================================================
# 脚本入口
# ============================================================================
# 【Python 语法说明】
# if __name__ == "__main__":
#     这是 Python 的标准写法,用于判断脚本是否被直接运行
#     - 如果直接运行(python analyze_moe_data.py),则 __name__ == "__main__"
#     - 如果被导入(import analyze_moe_data),则 __name__ == "analyze_moe_data"
#     这样可以避免在导入时自动执行 main() 函数

if __name__ == "__main__":
    main()  # 调用主函数
