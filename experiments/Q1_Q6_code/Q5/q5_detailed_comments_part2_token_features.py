#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ============================================================================
# Q5 路由置信度特征分析脚本 - 详细注释补充（第2部分）
# Token 级置信度特征提取
# ============================================================================

"""
========================================================================
本部分包含 Q5 的核心函数：
1. extract_token_confidence：提取 token 级置信度特征
   - Top-1 概率
   - Margin（Top-1 与 Top-2 差值）
   - 归一化熵
========================================================================
"""

# ============================================================================
# 核心函数：extract_token_confidence（Token 级置信度特征提取）
# ============================================================================
def extract_token_confidence(probs: np.ndarray) -> Dict[str, np.ndarray]:
    """
    ========================================================================
    函数功能：从 Top-K 概率矩阵中提取 token 级置信度特征
    ========================================================================

    【什么是置信度特征？】
    置信度特征用于衡量"路由有多自信"，包括3个核心指标：
    1. Top-1 概率：最高路由概率
    2. Margin：Top-1 与 Top-2 的差值
    3. 归一化熵：路由的不确定性

    【给零基础同学的解释】：
    想象你要评估一个学生选课的"确定性"：
    - Top-1 概率：学生最喜欢的课的分数（例如数学90分）
    - Margin：最喜欢和第二喜欢的差距（数学90分 - 物理70分 = 20分）
    - 熵：学生对所有课程的"犹豫程度"

    【为什么需要这些特征？】
    1. 预测任务难度：置信度低 → 任务可能更难
    2. 预测答对/答错：置信度低的样本更可能答错
    3. 分析路由行为：不同任务的路由置信度差异

    【三个特征的含义】：

    **1. Top-1 概率（最高路由概率）**：
    - 含义：路由给最优专家的概率
    - 范围：[0, 1]
    - 解释：
      * Top-1 = 0.9：路由很自信（90%选这个专家）
      * Top-1 = 0.3：路由不自信（只有30%选这个专家）

    **2. Margin（Top-1 与 Top-2 差值）**：
    - 含义：最优专家和次优专家的概率差
    - 范围：[0, 1]
    - 解释：
      * Margin = 0.6：Top-1 明显优于 Top-2（例如 0.8 vs 0.2）
      * Margin = 0.05：Top-1 和 Top-2 接近（例如 0.35 vs 0.30）

    **3. 归一化熵（路由不确定性）**：
    - 含义：路由在多个专家之间的"犹豫程度"
    - 范围：[0, 1]
    - 解释：
      * 熵 = 0：完全确定（某个专家概率100%）
      * 熵 = 1：完全不确定（所有专家概率相同）

    【示例】：
    假设有 3 个 token，每个 token 的 Top-4 概率：

    Token 0（很自信）：
    probs[0] = [0.8, 0.1, 0.05, 0.05]
    - Top-1 概率：0.8（很高）
    - Margin：0.8 - 0.1 = 0.7（差距很大）
    - 熵：较低（集中在一个专家）

    Token 1（比较犹豫）：
    probs[1] = [0.4, 0.3, 0.2, 0.1]
    - Top-1 概率：0.4（中等）
    - Margin：0.4 - 0.3 = 0.1（差距较小）
    - 熵：较高（分散在多个专家）

    Token 2（非常犹豫）：
    probs[2] = [0.25, 0.25, 0.25, 0.25]
    - Top-1 概率：0.25（很低）
    - Margin：0.25 - 0.25 = 0（没有差距）
    - 熵：最高（完全均匀）

    【给零基础同学的解释】：
    想象你要评估学生选课的"确定性"：
    - 学生A：数学90分，物理60分，化学50分 → 很确定选数学
    - 学生B：数学70分，物理65分，化学60分 → 比较犹豫
    - 学生C：数学60分，物理60分，化学60分 → 完全不知道选哪个

    【参数说明】：
    - probs: np.ndarray，形状 (n_tokens, top_k)
      * n_tokens：token 数量
      * top_k：Top-K 专家数量（通常是6）
      * probs[t, k]：第 t 个 token 的第 k 个候选专家的概率

    【返回值】：
    - Dict[str, np.ndarray]，包含3个特征：
      * "top1_prob": shape (n_tokens,) - Top-1 概率
      * "margin": shape (n_tokens,) - Margin
      * "entropy": shape (n_tokens,) - 归一化熵
    """
    # ====================================================================
    # 步骤1：获取数据维度
    # ====================================================================
    """
    【获取维度】：
    n_tokens = probs.shape[0]：token 数量
    top_k = probs.shape[1]：Top-K 数量

    【示例】：
    probs.shape = (500, 6)
    - n_tokens = 500（500个 token）
    - top_k = 6（Top-6 专家）

    【边界情况】：
    如果 probs 是一维数组（只有1个 token）：
    - len(probs.shape) == 1
    - top_k = 1

    【给零基础同学的解释】：
    想象你有一张表格：
    - 行数：有多少个 token
    - 列数：每个 token 有多少个候选专家
    """
    n_tokens = probs.shape[0]
    top_k = probs.shape[1] if len(probs.shape) > 1 else 1

    # ====================================================================
    # 步骤2：提取 Top-1 概率
    # ====================================================================
    """
    【提取 Top-1 概率】：
    top1_prob = np.max(probs, axis=1)

    【为什么用 max？】
    因为 Top-K 概率不一定是排序好的，需要找最大值。

    【axis=1 的含义】：
    按行取最大值（对每个 token，找最大概率）。

    【示例】：
    probs = [[0.8, 0.1, 0.05, 0.05],
             [0.4, 0.3, 0.2, 0.1],
             [0.25, 0.25, 0.25, 0.25]]

    top1_prob = np.max(probs, axis=1)
    top1_prob = [0.8, 0.4, 0.25]

    【边界情况】：
    如果 top_k == 1（只有1个候选）：
    - top1_prob = probs.flatten()（直接展平）

    【给零基础同学的解释】：
    想象你要找每个学生的"最高分"：
    - 学生A：[90, 85, 80] → 最高分 = 90
    - 学生B：[70, 65, 60] → 最高分 = 70
    - 学生C：[60, 60, 60] → 最高分 = 60
    """
    top1_prob = np.max(probs, axis=1) if top_k > 1 else probs.flatten()

    # ====================================================================
    # 步骤3：计算 Margin（Top-1 与 Top-2 差值）
    # ====================================================================
    """
    【计算 Margin】：
    Margin = Top-1 概率 - Top-2 概率

    【为什么需要 Margin？】
    因为 Top-1 概率高不一定代表"自信"：
    - 情况1：Top-1 = 0.8，Top-2 = 0.1 → Margin = 0.7（很自信）
    - 情况2：Top-1 = 0.4，Top-2 = 0.35 → Margin = 0.05（不自信）

    Margin 衡量"最优选择和次优选择的差距"。

    【实现步骤】：
    1. 对每个 token 的概率降序排序
    2. 取前2个：sorted_probs[:, 0]（Top-1）和 sorted_probs[:, 1]（Top-2）
    3. 计算差值

    【代码详解】：
    sorted_probs = np.sort(probs, axis=1)[:, ::-1]

    【逐步拆解】：
    1. np.sort(probs, axis=1)：按行排序（升序）
    2. [:, ::-1]：反转列（变成降序）

    【示例】：
    probs = [[0.1, 0.8, 0.05, 0.05],
             [0.3, 0.4, 0.2, 0.1],
             [0.25, 0.25, 0.25, 0.25]]

    步骤1：升序排序
    [[0.05, 0.05, 0.1, 0.8],
     [0.1, 0.2, 0.3, 0.4],
     [0.25, 0.25, 0.25, 0.25]]

    步骤2：反转（降序）
    sorted_probs = [[0.8, 0.1, 0.05, 0.05],
                    [0.4, 0.3, 0.2, 0.1],
                    [0.25, 0.25, 0.25, 0.25]]

    步骤3：计算 Margin
    margin = sorted_probs[:, 0] - sorted_probs[:, 1]
    margin = [0.8-0.1, 0.4-0.3, 0.25-0.25]
    margin = [0.7, 0.1, 0.0]

    【边界情况】：
    如果 top_k < 2（只有1个候选）：
    - margin = np.ones(n_tokens)（全部设为1）
    - 因为没有 Top-2，无法计算差值

    【给零基础同学的解释】：
    想象你要计算"第一名和第二名的差距"：
    - 学生A：第一名90分，第二名85分 → 差距 = 5分
    - 学生B：第一名70分，第二名68分 → 差距 = 2分
    - 差距大 → 第一名明显更优
    - 差距小 → 第一名和第二名接近
    """
    if top_k >= 2:
        sorted_probs = np.sort(probs, axis=1)[:, ::-1]  # 降序排列
        margin = sorted_probs[:, 0] - sorted_probs[:, 1]
    else:
        margin = np.ones(n_tokens)

    # ====================================================================
    # 步骤4：计算归一化熵
    # ====================================================================
    """
    【计算熵】：
    entropies = np.array([normalized_entropy(probs[t]) for t in range(n_tokens)])

    【为什么要逐个计算？】
    因为 normalized_entropy 函数接受一维数组（单个 token 的概率）。

    【列表推导式详解】：
    [normalized_entropy(probs[t]) for t in range(n_tokens)]

    【逐步拆解】：
    1. range(n_tokens)：生成 [0, 1, 2, ..., n_tokens-1]
    2. probs[t]：第 t 个 token 的 Top-K 概率
    3. normalized_entropy(probs[t])：计算该 token 的归一化熵
    4. [...]：收集所有结果到列表
    5. np.array(...)：转换为 NumPy 数组

    【示例】：
    probs = [[0.8, 0.1, 0.05, 0.05],
             [0.4, 0.3, 0.2, 0.1],
             [0.25, 0.25, 0.25, 0.25]]

    计算过程：
    - t=0：normalized_entropy([0.8, 0.1, 0.05, 0.05]) ≈ 0.31
    - t=1：normalized_entropy([0.4, 0.3, 0.2, 0.1]) ≈ 0.68
    - t=2：normalized_entropy([0.25, 0.25, 0.25, 0.25]) = 1.0

    entropies = [0.31, 0.68, 1.0]

    【给零基础同学的解释】：
    想象你要计算每个学生的"犹豫程度"：
    - 学生A：数学90分，其他都很低 → 犹豫程度低（0.31）
    - 学生B：数学70分，物理65分，化学60分 → 犹豫程度中等（0.68）
    - 学生C：所有课都60分 → 犹豫程度最高（1.0）
    """
    entropies = np.array([normalized_entropy(probs[t]) for t in range(n_tokens)])

    # ====================================================================
    # 步骤5：返回特征字典
    # ====================================================================
    """
    【返回结果】：
    return {
        "top1_prob": top1_prob,
        "margin": margin,
        "entropy": entropies,
    }

    【返回值格式】：
    字典，包含3个键值对：
    - "top1_prob": np.ndarray，形状 (n_tokens,)
    - "margin": np.ndarray，形状 (n_tokens,)
    - "entropy": np.ndarray，形状 (n_tokens,)

    【示例】：
    {
        "top1_prob": array([0.8, 0.4, 0.25]),
        "margin": array([0.7, 0.1, 0.0]),
        "entropy": array([0.31, 0.68, 1.0]),
    }

    【给零基础同学的解释】：
    想象你要整理每个学生的评估结果：
    - 学生A：最高分=90，差距=5，犹豫度=0.31
    - 学生B：最高分=70，差距=2，犹豫度=0.68
    - 学生C：最高分=60，差距=0，犹豫度=1.0
    """
    return {
        "top1_prob": top1_prob,
        "margin": margin,
        "entropy": entropies,
    }


# ============================================================================
# Q5 Part 2 完成
# ============================================================================
# 下一部分将包含：
# - aggregate_sample_features 函数的详细注释
# - 样本级特征聚合的详细解释
# - 低置信度比例的计算方法
# ============================================================================
