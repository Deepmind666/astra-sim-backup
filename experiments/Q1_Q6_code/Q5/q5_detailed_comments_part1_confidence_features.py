#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ============================================================================
# Q5 路由置信度特征分析脚本 - 详细注释补充（第1部分）
# 核心数学函数和置信度特征提取
# ============================================================================

"""
========================================================================
本部分包含 Q5 的核心函数：
1. entropy：计算分布熵
2. normalized_entropy：计算归一化熵
3. extract_token_confidence：提取 token 级置信度特征
4. aggregate_sample_features：聚合为样本级特征
========================================================================
"""

# ============================================================================
# 核心数学函数：entropy（熵）
# ============================================================================
def entropy(p: np.ndarray, eps: float = 1e-12) -> float:
    """
    ========================================================================
    函数功能：计算概率分布的熵（Entropy）
    ========================================================================

    【什么是熵？】
    熵是信息论中的概念，用于衡量"不确定性"或"混乱程度"。

    【给零基础同学的解释】：
    想象你要猜一个学生会选哪门课：
    - 情况1：学生90%选数学，10%选物理 → 很确定（熵低）
    - 情况2：学生50%选数学，50%选物理 → 不确定（熵高）

    熵就是衡量"你有多不确定"的数字：
    - 熵 = 0：完全确定（100%选某一个）
    - 熵 = 最大值：完全不确定（所有选项概率相同）

    【在 MoE 路由中的含义】：
    - 熵低：路由很"自信"（某个专家概率很高）
    - 熵高：路由很"犹豫"（多个专家概率接近）

    【数学公式】：
    H(P) = -Σ P(i) * log2(P(i))

    其中：
    - P(i)：第 i 个专家的概率
    - log2：以2为底的对数（信息论标准）

    【示例】：
    假设有 4 个专家，Top-4 概率为：

    情况1（很确定）：
    P = [0.9, 0.05, 0.03, 0.02]

    【逐步计算】：
    H = -(0.9*log2(0.9) + 0.05*log2(0.05) + 0.03*log2(0.03) + 0.02*log2(0.02))
    H = -(0.9*(-0.152) + 0.05*(-4.322) + 0.03*(-5.059) + 0.02*(-5.644))
    H = -(-0.137 - 0.216 - 0.152 - 0.113)
    H = 0.618（熵较低，比较确定）

    情况2（很犹豫）：
    P = [0.25, 0.25, 0.25, 0.25]

    【逐步计算】：
    H = -(0.25*log2(0.25) + 0.25*log2(0.25) + 0.25*log2(0.25) + 0.25*log2(0.25))
    H = -(4 * 0.25 * (-2))
    H = 2.0（熵最大，完全不确定）

    【结果解释】：
    - 情况1：H = 0.618（路由比较自信）
    - 情况2：H = 2.0（路由完全犹豫）

    【给零基础同学的解释】：
    想象你要猜一个骰子的结果：
    - 如果骰子被做了手脚，90%会出现6 → 熵低（容易猜）
    - 如果骰子是公平的，每面概率相同 → 熵高（难猜）

    【参数说明】：
    - p: np.ndarray，概率分布
    - eps: float，防止 log(0) 的小常数（默认 1e-12）

    【返回值】：
    - float，熵值（单位：bit）
    """
    # ====================================================================
    # 步骤1：转换为 NumPy 数组
    # ====================================================================
    """
    【转换为数组】：
    p = np.asarray(p, dtype=np.float64)

    【为什么要转换？】
    确保输入是 NumPy 数组，方便后续计算。

    【给零基础同学的解释】：
    想象你要把一串数字整理成统一格式，方便计算。
    """
    p = np.asarray(p, dtype=np.float64)

    # ====================================================================
    # 步骤2：截断概率值，避免 log(0)
    # ====================================================================
    """
    【截断操作】：
    p = np.clip(p, eps, 1.0)

    【为什么要截断？】
    因为 log(0) = -∞，会导致计算错误。

    【np.clip 的作用】：
    将数组中的值限制在 [eps, 1.0] 范围内：
    - 如果 p[i] < eps，则设为 eps
    - 如果 p[i] > 1.0，则设为 1.0
    - 否则保持不变

    【示例】：
    p = [0.9, 0.0, 0.05, 0.05]
    eps = 1e-12
    np.clip(p, eps, 1.0) = [0.9, 1e-12, 0.05, 0.05]

    【给零基础同学的解释】：
    想象你要计算 log(概率)：
    - 如果概率 = 0，log(0) 会报错
    - 所以我们把 0 替换成一个很小的数（0.000000000001）
    """
    p = np.clip(p, eps, 1.0)

    # ====================================================================
    # 步骤3：归一化，确保概率和为 1
    # ====================================================================
    """
    【归一化】：
    p = _safe_normalize(p)

    【为什么要归一化？】
    因为概率分布的定义要求：Σ P(i) = 1

    【_safe_normalize 的作用】：
    将数组归一化为概率分布（和为1）。

    【给零基础同学的解释】：
    想象你有一堆概率，但总和不是100%：
    - 原来：[0.9, 0.05, 0.03, 0.02]，总和 = 1.0（已经是100%）
    - 如果总和不是1，就按比例调整
    """
    p = _safe_normalize(p)

    # ====================================================================
    # 步骤4：计算熵
    # ====================================================================
    """
    【熵的计算】：
    H = -Σ P(i) * log2(P(i))

    【代码实现】：
    return float(-np.sum(p * np.log2(p)))

    【逐步拆解】：
    1. np.log2(p)：计算每个概率的 log2
       例如：log2([0.9, 0.05, 0.03, 0.02]) = [-0.152, -4.322, -5.059, -5.644]

    2. p * np.log2(p)：加权
       例如：[0.9, 0.05, 0.03, 0.02] * [-0.152, -4.322, -5.059, -5.644]
            = [-0.137, -0.216, -0.152, -0.113]

    3. np.sum(...)：求和
       例如：-0.137 + (-0.216) + (-0.152) + (-0.113) = -0.618

    4. -...：取负号
       例如：-(-0.618) = 0.618

    【给零基础同学的解释】：
    想象你要计算"不确定性"：
    - 每个选项的"惊讶度" = -log2(概率)
      * 概率高 → 惊讶度低（不意外）
      * 概率低 → 惊讶度高（很意外）
    - 总不确定性 = 加权平均惊讶度
    """
    return float(-np.sum(p * np.log2(p)))


# ============================================================================
# 核心函数：normalized_entropy（归一化熵）
# ============================================================================
def normalized_entropy(p: np.ndarray, eps: float = 1e-12) -> float:
    """
    ========================================================================
    函数功能：计算归一化熵（Normalized Entropy）
    ========================================================================

    【什么是归一化熵？】
    归一化熵是把熵除以"最大可能熵"，得到 [0, 1] 范围的值。

    【给零基础同学的解释】：
    想象你要比较两个场景的"不确定性"：
    - 场景1：从4个选项中选（最大熵 = log2(4) = 2）
    - 场景2：从8个选项中选（最大熵 = log2(8) = 3）

    如果直接比较熵值，不公平（选项多的熵天然更大）。
    归一化熵就是"除以最大熵"，让两个场景可以公平比较。

    【数学公式】：
    Normalized_H = H / log2(K)

    其中：
    - H：实际熵
    - K：选项数量（专家数量）
    - log2(K)：最大可能熵（均匀分布的熵）

    【示例】：
    假设有 4 个专家，Top-4 概率为：

    情况1（很确定）：
    P = [0.9, 0.05, 0.03, 0.02]
    H = 0.618（实际熵）
    最大熵 = log2(4) = 2.0
    归一化熵 = 0.618 / 2.0 = 0.309（30.9% 的最大不确定性）

    情况2（很犹豫）：
    P = [0.25, 0.25, 0.25, 0.25]
    H = 2.0（实际熵）
    最大熵 = log2(4) = 2.0
    归一化熵 = 2.0 / 2.0 = 1.0（100% 的最大不确定性）

    【结果解释】：
    - 归一化熵 = 0：完全确定（某个专家概率100%）
    - 归一化熵 = 1：完全不确定（所有专家概率相同）

    【给零基础同学的解释】：
    想象你要比较两个考试的"难度"：
    - 考试1：4道选择题，平均答对率60% → 难度 = 40%
    - 考试2：8道选择题，平均答对率50% → 难度 = 50%

    归一化熵就是"把不同规模的不确定性统一到 [0, 1] 范围"。

    【参数说明】：
    - p: np.ndarray，概率分布
    - eps: float，防止 log(0) 的小常数

    【返回值】：
    - float，归一化熵，范围 [0, 1]
    """
    # ====================================================================
    # 步骤1：转换为 NumPy 数组
    # ====================================================================
    p = np.asarray(p, dtype=np.float64)

    # ====================================================================
    # 步骤2：获取选项数量
    # ====================================================================
    """
    【获取选项数量】：
    k = len(p)

    【示例】：
    p = [0.9, 0.05, 0.03, 0.02]
    k = 4（4个专家）

    【给零基础同学的解释】：
    想象你要数有多少个选项（专家）。
    """
    k = len(p)

    # ====================================================================
    # 步骤3：边界情况处理
    # ====================================================================
    """
    【边界情况】：
    if k <= 1:
        return 0.0

    【为什么返回 0？】
    因为只有1个选项时，没有不确定性（100%选这个）。

    【示例】：
    p = [1.0]（只有1个专家）
    归一化熵 = 0（完全确定）

    【给零基础同学的解释】：
    想象你只有1个选项：
    → 没有选择余地，不确定性 = 0
    """
    if k <= 1:
        return 0.0

    # ====================================================================
    # 步骤4：计算实际熵
    # ====================================================================
    """
    【计算熵】：
    h = entropy(p, eps)

    【调用前面定义的 entropy 函数】。

    【给零基础同学的解释】：
    想象你要先计算"原始不确定性"。
    """
    h = entropy(p, eps)

    # ====================================================================
    # 步骤5：计算最大熵
    # ====================================================================
    """
    【最大熵】：
    max_h = math.log2(k)

    【为什么是 log2(k)？】
    因为均匀分布的熵 = log2(选项数量)。

    【示例】：
    k = 4
    max_h = log2(4) = 2.0

    k = 8
    max_h = log2(8) = 3.0

    【给零基础同学的解释】：
    想象你要计算"最大可能不确定性"：
    - 4个选项，每个25% → 最大熵 = log2(4) = 2
    - 8个选项，每个12.5% → 最大熵 = log2(8) = 3
    """
    max_h = math.log2(k)

    # ====================================================================
    # 步骤6：归一化
    # ====================================================================
    """
    【归一化】：
    return h / max_h if max_h > 0 else 0.0

    【为什么要除以 max_h？】
    把熵归一化到 [0, 1] 范围。

    【示例】：
    h = 0.618
    max_h = 2.0
    归一化熵 = 0.618 / 2.0 = 0.309

    【给零基础同学的解释】：
    想象你要把"不确定性"转换为百分比：
    - 实际不确定性：0.618
    - 最大不确定性：2.0
    - 百分比：0.618 / 2.0 = 30.9%
    """
    return h / max_h if max_h > 0 else 0.0


# ============================================================================
# Q5 Part 1 完成
# ============================================================================
# 下一部分将包含：
# - extract_token_confidence 函数的详细注释
# - aggregate_sample_features 函数的详细注释
# - Top-1 概率、Margin、熵的详细解释
# ============================================================================
