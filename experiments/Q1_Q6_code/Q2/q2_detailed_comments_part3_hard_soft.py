#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ============================================================================
# Q2 任务偏好分析脚本 - 详细注释补充（第3部分）
# build_token_vectors 函数：Hard/Soft 口径的详细解释
# ============================================================================

"""
========================================================================
本部分重点解释 Q2 的核心概念：Hard 口径 vs Soft 口径
========================================================================
"""

# ============================================================================
# 核心函数：build_token_vectors（构造 Token 向量）
# ============================================================================
def build_token_vectors(
    layer_names: List[str],
    probs_by_layer: Dict[str, np.ndarray],
    inds_by_layer: Dict[str, np.ndarray],
    idxs: np.ndarray,
    n_experts: int,
) -> Tuple[np.ndarray, np.ndarray, Dict[str, np.ndarray]]:
    """
    ========================================================================
    函数功能：为每个 token 构造专家使用向量（Hard 和 Soft 两种口径）
    ========================================================================

    【核心问题】：
    如何量化"一个 token 使用了哪些专家"？

    【两种口径】：
    1. Hard 口径：每层只看 Top-1 专家（最保守）
    2. Soft 口径：考虑 Top-(K+2) 的概率分布（更全面）

    【给零基础同学的解释】：
    想象你要统计"一个学生选了哪些课程"：

    Hard 口径（只看主修）：
    - 第1学期：主修数学
    - 第2学期：主修物理
    - 第3学期：主修化学
    → 统计：数学1次，物理1次，化学1次

    Soft 口径（考虑选修）：
    - 第1学期：数学（主修，权重0.6），物理（选修，权重0.3），化学（选修，权重0.1）
    - 第2学期：物理（主修，权重0.7），数学（选修，权重0.2），化学（选修，权重0.1）
    - 第3学期：化学（主修，权重0.5），数学（选修，权重0.3），物理（选修，权重0.2）
    → 统计：数学1.1次，物理1.2次，化学0.7次

    【为什么需要两种口径？】
    - Hard 口径：简单、直观，但可能丢失信息
    - Soft 口径：更全面，但依赖概率估计的准确性

    【数学公式】：

    Hard 口径：
    H_e = (1 / L) * Σ_{l=1}^{L} 1[Top1_l = e]

    其中：
    - L：MoE 层数（例如 24 层）
    - Top1_l：第 l 层的 Top-1 专家
    - 1[...]：指示函数（条件为真时=1，否则=0）
    - H_e：专家 e 的 Hard 使用频率

    Soft 口径：
    S_e = (1 / L) * Σ_{l=1}^{L} Σ_{k=1}^{K+2} P_l^k * 1[Ind_l^k = e]

    其中：
    - P_l^k：第 l 层第 k 个专家的概率
    - Ind_l^k：第 l 层第 k 个专家的索引
    - S_e：专家 e 的 Soft 使用频率

    【公式详解（Hard 口径）】：
    假设有 3 个 MoE 层，60 个专家，某个 token 的路由情况：
    - 层1：Top-1 = 专家5
    - 层2：Top-1 = 专家12
    - 层3：Top-1 = 专家5

    Hard 向量（60维）：
    - 专家5：2/3 ≈ 0.667（在3层中被选为Top-1两次）
    - 专家12：1/3 ≈ 0.333（在3层中被选为Top-1一次）
    - 其他专家：0

    【公式详解（Soft 口径）】：
    假设同样的 token，但考虑 Top-3 概率：
    - 层1：专家5(0.6), 专家8(0.3), 专家12(0.1)
    - 层2：专家12(0.5), 专家5(0.3), 专家20(0.2)
    - 层3：专家5(0.7), 专家8(0.2), 专家12(0.1)

    Soft 向量（60维）：
    - 专家5：(0.6 + 0.3 + 0.7) / 3 = 1.6 / 3 ≈ 0.533
    - 专家12：(0.1 + 0.5 + 0.1) / 3 = 0.7 / 3 ≈ 0.233
    - 专家8：(0.3 + 0 + 0.2) / 3 = 0.5 / 3 ≈ 0.167
    - 专家20：(0 + 0.2 + 0) / 3 = 0.2 / 3 ≈ 0.067
    - 其他专家：0

    【参数说明】：
    - layer_names: List[str]，MoE 层名列表（例如 ['MoE_Gate0', 'MoE_Gate1', ...]）
    - probs_by_layer: Dict[str, np.ndarray]，每层的 Top-(K+2) 概率
      格式：{'MoE_Gate0': array([[0.6, 0.3, 0.1], ...]), ...}
    - inds_by_layer: Dict[str, np.ndarray]，每层的 Top-(K+2) 专家索引
      格式：{'MoE_Gate0': array([[5, 8, 12], ...]), ...}
    - idxs: np.ndarray，选中的 token 索引（用于抽样）
    - n_experts: int，专家总数（例如 60）

    【返回值】：
    - hard_vecs: np.ndarray，Hard 向量，shape (n_tokens, n_experts)
    - soft_vecs: np.ndarray，Soft 向量，shape (n_tokens, n_experts)
    - stats: Dict[str, np.ndarray]，置信度统计
      包含：top1_prob（Top-1概率），margin（Top-1与Top-2差值），entropy（熵）
    """
    # ====================================================================
    # 步骤1：初始化数据结构
    # ====================================================================
    """
    【数据结构说明】：
    - n_tokens：选中的 token 数量
    - n_layers：MoE 层数
    - hard_vecs：Hard 向量矩阵，shape (n_tokens, n_experts)
    - soft_vecs：Soft 向量矩阵，shape (n_tokens, n_experts)

    【给零基础同学的解释】：
    想象你要统计 1000 个学生选了哪些课程（60门课）：
    - 准备一个表格：1000行（学生）× 60列（课程）
    - 每个格子记录"该学生选该课程的频率"
    """
    n_tokens = idxs.shape[0]  # token 数
    n_layers = len(layer_names)  # 层数
    hard_vecs = np.zeros((n_tokens, n_experts), dtype=np.float64)  # Hard 初始化
    soft_vecs = np.zeros((n_tokens, n_experts), dtype=np.float64)  # Soft 初始化

    # 初始化置信度统计
    top1_prob = np.zeros((n_tokens, n_layers), dtype=np.float64)
    margin = np.zeros((n_tokens, n_layers), dtype=np.float64)
    entropy = np.zeros((n_tokens, n_layers), dtype=np.float64)

    # ====================================================================
    # 步骤2：遍历每一层，累积专家使用频率
    # ====================================================================
    """
    【循环逻辑】：
    for li, ln in enumerate(layer_names):
        # 处理第 li 层（层名为 ln）

    【enumerate 的作用】：
    同时获取索引和值：
    - li：层索引（0, 1, 2, ...）
    - ln：层名（'MoE_Gate0', 'MoE_Gate1', ...）

    【给零基础同学的解释】：
    想象你要统计每学期的选课情况：
    - 第1学期（li=0）：统计主修和选修
    - 第2学期（li=1）：统计主修和选修
    - ...
    """
    for li, ln in enumerate(layer_names):
        # ================================================================
        # 步骤2.1：提取该层的数据
        # ================================================================
        """
        【数据提取】：
        probs = probs_by_layer[ln][idxs]：提取选中 token 的概率
        inds = inds_by_layer[ln][idxs]：提取选中 token 的专家索引

        【示例】：
        假设 idxs = [0, 5, 10]（选中第0、5、10个token）
        probs_by_layer['MoE_Gate0'] = array([[0.6, 0.3, 0.1], [0.5, 0.3, 0.2], ...])
        probs = array([[0.6, 0.3, 0.1], [0.5, 0.3, 0.2], [0.7, 0.2, 0.1]])
        """
        probs = probs_by_layer[ln][idxs]
        inds = inds_by_layer[ln][idxs]

        # ================================================================
        # 步骤2.2：归一化概率
        # ================================================================
        """
        【为什么要归一化？】
        因为 Top-(K+2) 的概率和可能不等于 1。

        【_normalize_rows 函数】：
        对每一行做归一化，使其和为 1。

        【示例】：
        原始：[0.6, 0.3, 0.1]（和=1.0，已归一化）
        原始：[0.5, 0.3, 0.1]（和=0.9，需要归一化）
        归一化后：[0.556, 0.333, 0.111]（和=1.0）
        """
        probs_norm = _normalize_rows(probs.astype(np.float64))


# ============================================================================
# 第3部分完成
# ============================================================================
# 下一部分将包含：
# - Hard 口径的具体实现（Top-1 计票）
# - Soft 口径的具体实现（概率累加）
# - 置信度特征的计算（Top-1概率、Margin、熵）
# ============================================================================
