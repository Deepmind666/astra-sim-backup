#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ============================================================================
# Q2 任务偏好分析脚本 - 数学函数详细注释补充（第1部分）
# ============================================================================

"""
========================================================================
本文件是对 q2_analyze_task_specialization_v1.py 的注释补充
重点补充数学公式的详细推导和零基础友好的解释
========================================================================
"""

# ============================================================================
# 核心数学函数：JSD（Jensen-Shannon Divergence）
# ============================================================================
def jsd(p: np.ndarray, q: np.ndarray) -> float:
    """
    ========================================================================
    函数功能：计算两个概率分布之间的 Jensen-Shannon Divergence（JSD）
    ========================================================================

    【什么是 JSD？】
    JSD 是一种衡量"两个概率分布有多不同"的指标，基于信息论。

    【给零基础同学的解释】：
    想象你有两个班级的成绩分布：
    - 班级A：90分的人多，60分的人少
    - 班级B：60分的人多，90分的人少

    JSD 就是衡量"这两个分布有多不同"的数字：
    - JSD = 0：两个分布完全相同
    - JSD = 1：两个分布完全不同

    【数学公式】：
    JSD(P||Q) = 0.5 * KL(P||M) + 0.5 * KL(Q||M)

    其中：
    - M = 0.5 * (P + Q)：两个分布的平均
    - KL(P||M)：P 相对于 M 的 KL 散度
    - KL(Q||M)：Q 相对于 M 的 KL 散度

    【KL 散度公式】：
    KL(P||M) = Σ P(i) * log2(P(i) / M(i))

    【公式详解】：
    假设有 3 个专家，两个任务的专家使用分布为：
    - 任务A：P = [0.5, 0.3, 0.2]（专家0用50%，专家1用30%，专家2用20%）
    - 任务B：Q = [0.2, 0.3, 0.5]（专家0用20%，专家1用30%，专家2用50%）

    【逐步计算】：
    1. 计算中间分布 M：
       M = 0.5 * (P + Q)
       M = 0.5 * ([0.5, 0.3, 0.2] + [0.2, 0.3, 0.5])
       M = 0.5 * [0.7, 0.6, 0.7]
       M = [0.35, 0.30, 0.35]

    2. 计算 KL(P||M)：
       KL(P||M) = P[0]*log2(P[0]/M[0]) + P[1]*log2(P[1]/M[1]) + P[2]*log2(P[2]/M[2])
       KL(P||M) = 0.5*log2(0.5/0.35) + 0.3*log2(0.3/0.30) + 0.2*log2(0.2/0.35)
       KL(P||M) = 0.5*log2(1.43) + 0.3*log2(1.0) + 0.2*log2(0.57)
       KL(P||M) = 0.5*0.515 + 0.3*0 + 0.2*(-0.807)
       KL(P||M) = 0.258 + 0 - 0.161
       KL(P||M) ≈ 0.097

    3. 计算 KL(Q||M)：
       KL(Q||M) = Q[0]*log2(Q[0]/M[0]) + Q[1]*log2(Q[1]/M[1]) + Q[2]*log2(Q[2]/M[2])
       KL(Q||M) = 0.2*log2(0.2/0.35) + 0.3*log2(0.3/0.30) + 0.5*log2(0.5/0.35)
       KL(Q||M) = 0.2*log2(0.57) + 0.3*log2(1.0) + 0.5*log2(1.43)
       KL(Q||M) = 0.2*(-0.807) + 0.3*0 + 0.5*0.515
       KL(Q||M) = -0.161 + 0 + 0.258
       KL(Q||M) ≈ 0.097

    4. 计算 JSD：
       JSD(P||Q) = 0.5 * KL(P||M) + 0.5 * KL(Q||M)
       JSD(P||Q) = 0.5 * 0.097 + 0.5 * 0.097
       JSD(P||Q) ≈ 0.097

    【结果解释】：
    JSD ≈ 0.097 表示两个任务的专家使用分布有一定差异，但不是完全不同。

    【为什么用 JSD 而不是其他距离？】
    1. 对称性：JSD(P||Q) = JSD(Q||P)（KL 散度不对称）
    2. 有界性：JSD 的范围是 [0, 1]（方便比较）
    3. 信息论意义：衡量"信息差异"而不是"几何距离"

    【参数说明】：
    - p: np.ndarray，第一个概率分布（例如任务A的专家使用分布）
    - q: np.ndarray，第二个概率分布（例如任务B的专家使用分布）

    【返回值】：
    - float，JSD 值，范围 [0, 1]
    """
    eps = 1e-12  # 防止 log(0) 导致 NaN

    # ====================================================================
    # 步骤1：截断概率值，避免 log(0)
    # ====================================================================
    """
    【为什么要截断？】
    因为 log(0) = -∞，会导致计算错误。

    【截断操作】：
    np.clip(p, eps, 1.0)：将 p 中的值限制在 [eps, 1.0] 范围内
    - 如果 p[i] < eps，则设为 eps
    - 如果 p[i] > 1.0，则设为 1.0
    - 否则保持不变

    【给零基础同学的解释】：
    想象你要计算 log(概率)：
    - 如果概率 = 0，log(0) 会报错
    - 所以我们把 0 替换成一个很小的数（0.000000000001）
    """
    p = np.clip(p, eps, 1.0)
    q = np.clip(q, eps, 1.0)

    # ====================================================================
    # 步骤2：归一化，确保概率和为 1
    # ====================================================================
    """
    【为什么要归一化？】
    因为概率分布的定义要求：Σ P(i) = 1

    【归一化操作】：
    p = p / p.sum()：将 p 中的每个值除以总和

    【示例】：
    假设 p = [2, 3, 5]（不是概率分布，和为10）
    归一化后：p = [2/10, 3/10, 5/10] = [0.2, 0.3, 0.5]（和为1）

    【给零基础同学的解释】：
    想象你有 10 个苹果，分给 3 个人：
    - 人1：2个（占20%）
    - 人2：3个（占30%）
    - 人3：5个（占50%）

    归一化就是把"个数"转换成"百分比"。
    """
    p = p / p.sum()
    q = q / q.sum()

    # ====================================================================
    # 步骤3：计算中间分布 M
    # ====================================================================
    """
    【中间分布 M】：
    M = 0.5 * (P + Q)：两个分布的平均

    【示例】：
    P = [0.5, 0.3, 0.2]
    Q = [0.2, 0.3, 0.5]
    M = 0.5 * ([0.5, 0.3, 0.2] + [0.2, 0.3, 0.5])
    M = 0.5 * [0.7, 0.6, 0.7]
    M = [0.35, 0.30, 0.35]

    【给零基础同学的解释】：
    想象两个班级的成绩分布：
    - 班级A：90分的人占50%，60分的人占30%，30分的人占20%
    - 班级B：90分的人占20%，60分的人占30%，30分的人占50%

    中间分布 M 就是"两个班级合并后的平均分布"。
    """
    m = 0.5 * (p + q)

    # ====================================================================
    # 步骤4：计算 KL 散度
    # ====================================================================
    """
    【KL 散度公式】：
    KL(P||M) = Σ P(i) * log2(P(i) / M(i))

    【代码实现】：
    kl_pm = np.sum(p * np.log2(p / m))

    【逐步拆解】：
    1. p / m：计算每个位置的比值
       例如：[0.5, 0.3, 0.2] / [0.35, 0.30, 0.35] = [1.43, 1.0, 0.57]

    2. np.log2(p / m)：计算 log2(比值)
       例如：log2([1.43, 1.0, 0.57]) = [0.515, 0, -0.807]

    3. p * np.log2(p / m)：加权
       例如：[0.5, 0.3, 0.2] * [0.515, 0, -0.807] = [0.258, 0, -0.161]

    4. np.sum(...)：求和
       例如：0.258 + 0 + (-0.161) = 0.097

    【给零基础同学的解释】：
    KL 散度衡量"P 相对于 M 有多不同"：
    - 如果 P 和 M 完全相同，KL = 0
    - 如果 P 和 M 差异很大，KL 很大
    """
    kl_pm = np.sum(p * np.log2(p / m))  # KL(P||M)
    kl_qm = np.sum(q * np.log2(q / m))  # KL(Q||M)

    # ====================================================================
    # 步骤5：计算 JSD
    # ====================================================================
    """
    【JSD 公式】：
    JSD(P||Q) = 0.5 * KL(P||M) + 0.5 * KL(Q||M)

    【为什么是 0.5？】
    因为 JSD 是"对称的平均"：
    - 0.5 * KL(P||M)：P 相对于 M 的差异
    - 0.5 * KL(Q||M)：Q 相对于 M 的差异

    【给零基础同学的解释】：
    想象你要衡量两个班级的成绩分布差异：
    - 先算班级A相对于平均的差异
    - 再算班级B相对于平均的差异
    - 最后取平均
    """
    return 0.5 * (kl_pm + kl_qm)


# ============================================================================
# 第1部分完成
# ============================================================================
# 下一部分将包含：
# - TV 距离的详细注释
# - build_token_vectors 函数的详细注释
# - Hard/Soft 口径的详细解释
# ============================================================================
