#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ============================================================================
# Q2 任务偏好分析脚本 - 数学函数详细注释补充（第2部分）
# TV 距离、集中度分数、Top-k 覆盖率
# ============================================================================

# ============================================================================
# 核心数学函数：TV（Total Variation Distance）
# ============================================================================
def tv(p: np.ndarray, q: np.ndarray) -> float:
    """
    ========================================================================
    函数功能：计算两个概率分布之间的 Total Variation Distance（TV 距离）
    ========================================================================

    【什么是 TV 距离？】
    TV 距离是衡量"两个概率分布有多不同"的另一种指标，基于概率论。

    【给零基础同学的解释】：
    想象你有两个班级的成绩分布：
    - 班级A：90分占50%，60分占30%，30分占20%
    - 班级B：90分占20%，60分占30%，30分占50%

    TV 距离就是"把班级A的分布调整成班级B，需要移动多少概率质量"。

    【数学公式】：
    TV(P, Q) = 0.5 * Σ |P(i) - Q(i)|

    【公式详解】：
    假设有 3 个专家，两个任务的专家使用分布为：
    - 任务A：P = [0.5, 0.3, 0.2]
    - 任务B：Q = [0.2, 0.3, 0.5]

    【逐步计算】：
    1. 计算每个位置的差值：
       P - Q = [0.5-0.2, 0.3-0.3, 0.2-0.5]
       P - Q = [0.3, 0.0, -0.3]

    2. 取绝对值：
       |P - Q| = [0.3, 0.0, 0.3]

    3. 求和：
       Σ |P - Q| = 0.3 + 0.0 + 0.3 = 0.6

    4. 乘以 0.5：
       TV(P, Q) = 0.5 * 0.6 = 0.3

    【结果解释】：
    TV = 0.3 表示"需要移动 30% 的概率质量才能把 P 变成 Q"。

    【TV 与 JSD 的区别】：
    - TV：基于概率论，衡量"概率质量移动距离"
    - JSD：基于信息论，衡量"信息差异"

    【为什么 TV 的范围是 [0, 1]？】
    - TV = 0：两个分布完全相同
    - TV = 1：两个分布完全不重叠（例如 P=[1,0,0], Q=[0,0,1]）

    【给零基础同学的解释】：
    想象你要把一堆苹果重新分配：
    - 原来：人1有5个，人2有3个，人3有2个
    - 目标：人1有2个，人2有3个，人3有5个

    TV 距离就是"需要移动多少个苹果"（占总数的比例）。

    【参数说明】：
    - p: np.ndarray，第一个概率分布
    - q: np.ndarray，第二个概率分布

    【返回值】：
    - float，TV 距离，范围 [0, 1]
    """
    # ====================================================================
    # 步骤1：计算差值的绝对值
    # ====================================================================
    """
    【代码详解】：
    np.abs(p - q)：计算每个位置的差值的绝对值

    【示例】：
    p = [0.5, 0.3, 0.2]
    q = [0.2, 0.3, 0.5]
    p - q = [0.3, 0.0, -0.3]
    np.abs(p - q) = [0.3, 0.0, 0.3]
    """
    diff_abs = np.abs(p - q)

    # ====================================================================
    # 步骤2：求和并乘以 0.5
    # ====================================================================
    """
    【代码详解】：
    0.5 * float(np.abs(p - q).sum())

    【为什么乘以 0.5？】
    因为 Σ |P(i) - Q(i)| 会"重复计算"：
    - 如果 P(i) > Q(i)，差值为正
    - 如果 P(i) < Q(i)，差值为负
    - 总和会把"增加的"和"减少的"都算一遍

    所以要除以 2（乘以 0.5）。

    【给零基础同学的解释】：
    想象你要移动苹果：
    - 从人1拿走3个（-3）
    - 给人3加上3个（+3）

    如果直接求和：|-3| + |+3| = 6（重复计算了）
    实际只移动了3个，所以要除以2。
    """
    return 0.5 * float(diff_abs.sum())


# ============================================================================
# 辅助函数：集中度分数（Concentration Score）
# ============================================================================
def conc_score(p: np.ndarray) -> float:
    """
    ========================================================================
    函数功能：计算概率分布的集中度分数
    ========================================================================

    【什么是集中度？】
    集中度衡量"概率质量是否集中在少数几个专家上"。

    【给零基础同学的解释】：
    想象你有 60 个专家：
    - 情况1：所有专家使用频率相同（均匀分布）→ 集中度低
    - 情况2：只有 3 个专家被频繁使用（集中分布）→ 集中度高

    【数学公式】：
    Concentration = 1 - H(p) / log(E)

    其中：
    - H(p)：分布的熵（自然对数）
    - E：专家总数
    - log(E)：最大熵（均匀分布的熵）

    【熵的公式】：
    H(p) = -Σ p(i) * ln(p(i))

    【公式详解】：
    假设有 3 个专家：

    情况1（均匀分布）：
    p = [1/3, 1/3, 1/3] = [0.333, 0.333, 0.333]
    H(p) = -(0.333*ln(0.333) + 0.333*ln(0.333) + 0.333*ln(0.333))
    H(p) = -(0.333*(-1.099) + 0.333*(-1.099) + 0.333*(-1.099))
    H(p) = -(-1.099) = 1.099
    log(3) = 1.099
    Concentration = 1 - 1.099/1.099 = 1 - 1 = 0（不集中）

    情况2（集中分布）：
    p = [0.9, 0.05, 0.05]
    H(p) = -(0.9*ln(0.9) + 0.05*ln(0.05) + 0.05*ln(0.05))
    H(p) = -(0.9*(-0.105) + 0.05*(-2.996) + 0.05*(-2.996))
    H(p) = -(-0.095 - 0.150 - 0.150) = 0.395
    Concentration = 1 - 0.395/1.099 = 1 - 0.36 = 0.64（很集中）

    【结果解释】：
    - Concentration = 0：完全均匀（所有专家使用频率相同）
    - Concentration = 1：完全集中（只有1个专家被使用）

    【参数说明】：
    - p: np.ndarray，概率分布

    【返回值】：
    - float，集中度分数，范围 [0, 1]
    """
    eps = 1e-12  # 防止 log(0)

    # ====================================================================
    # 步骤1：截断并归一化
    # ====================================================================
    p = np.clip(p, eps, 1.0)
    p = p / p.sum()

    # ====================================================================
    # 步骤2：计算熵
    # ====================================================================
    """
    【熵的计算】：
    H(p) = -Σ p(i) * ln(p(i))

    【代码实现】：
    ent = -np.sum(p * np.log(p))

    【给零基础同学的解释】：
    熵衡量"不确定性"：
    - 熵高：分布均匀，不确定性大
    - 熵低：分布集中，不确定性小
    """
    ent = -np.sum(p * np.log(p))

    # ====================================================================
    # 步骤3：计算集中度
    # ====================================================================
    """
    【集中度公式】：
    Concentration = 1 - H(p) / log(E)

    【为什么要除以 log(E)？】
    因为：
    - 均匀分布的熵 = log(E)（最大熵）
    - H(p) / log(E)：归一化到 [0, 1]
    - 1 - ...：反转（熵高 → 集中度低）

    【给零基础同学的解释】：
    想象你要衡量"专家使用是否集中"：
    - 如果熵 = 最大熵：完全均匀 → 集中度 = 0
    - 如果熵 = 0：完全集中 → 集中度 = 1
    """
    return 1.0 - ent / math.log(len(p))


# ============================================================================
# 辅助函数：Top-k 覆盖率曲线
# ============================================================================
def coverage_curve(p: np.ndarray, k_values: List[int]) -> Dict[int, float]:
    """
    ========================================================================
    函数功能：计算 Top-k 覆盖率曲线
    ========================================================================

    【什么是 Top-k 覆盖率？】
    Top-k 覆盖率表示"前 k 个最常用的专家，覆盖了多少比例的使用量"。

    【给零基础同学的解释】：
    想象你有 60 个专家，统计每个专家的使用频率：
    - 专家5：使用了 30% 的时间
    - 专家12：使用了 20% 的时间
    - 专家8：使用了 15% 的时间
    - 其他57个专家：共使用了 35% 的时间

    Top-3 覆盖率 = 30% + 20% + 15% = 65%
    （前3个专家覆盖了 65% 的使用量）

    【数学公式】：
    Coverage(k) = Σ (前k个最大的概率)

    【示例】：
    假设有 5 个专家，使用分布为：
    p = [0.3, 0.1, 0.25, 0.05, 0.3]

    【逐步计算】：
    1. 按概率降序排序：
       排序后：[0.3, 0.3, 0.25, 0.1, 0.05]
       对应专家：[0, 4, 2, 1, 3]

    2. 计算 Top-k 覆盖率：
       - Top-1：0.3（30%）
       - Top-2：0.3 + 0.3 = 0.6（60%）
       - Top-3：0.3 + 0.3 + 0.25 = 0.85（85%）
       - Top-4：0.3 + 0.3 + 0.25 + 0.1 = 0.95（95%）
       - Top-5：0.3 + 0.3 + 0.25 + 0.1 + 0.05 = 1.0（100%）

    【结果解释】：
    - Top-3 覆盖率 = 85%：前3个专家覆盖了 85% 的使用量
    - 说明负载比较集中（少数专家承担大部分工作）

    【参数说明】：
    - p: np.ndarray，概率分布
    - k_values: List[int]，要计算的 k 值列表（例如 [1, 3, 5, 10]）

    【返回值】：
    - Dict[int, float]，字典格式：{k: 覆盖率}
      例如：{1: 0.3, 3: 0.85, 5: 1.0}
    """
    # ====================================================================
    # 步骤1：按概率降序排序
    # ====================================================================
    """
    【代码详解】：
    order = np.argsort(p)[::-1]

    【np.argsort 的作用】：
    返回排序后的索引（升序）

    【示例】：
    p = [0.3, 0.1, 0.25, 0.05, 0.3]
    np.argsort(p) = [3, 1, 2, 0, 4]（升序索引）
    np.argsort(p)[::-1] = [4, 0, 2, 1, 3]（降序索引）

    【[::-1] 的含义】：
    反转数组（从升序变成降序）

    【给零基础同学的解释】：
    想象你要给学生成绩排名：
    - 原始成绩：[85, 60, 90, 75, 95]
    - 排名（降序）：[4, 2, 0, 3, 1]
      * 第1名：学生4（95分）
      * 第2名：学生2（90分）
      * 第3名：学生0（85分）
      * ...
    """
    order = np.argsort(p)[::-1]

    # ====================================================================
    # 步骤2：计算每个 k 的覆盖率
    # ====================================================================
    cov = {}  # 初始化字典
    for k in k_values:
        # ================================================================
        # 步骤2.1：防止 k 超出范围
        # ================================================================
        """
        【边界检查】：
        k = min(k, p.size)

        【为什么要检查？】
        如果 k > 专家总数，会导致索引越界。

        【示例】：
        如果只有 5 个专家，但 k=10，则取 k=5。
        """
        k = min(k, p.size)

        # ================================================================
        # 步骤2.2：累加前 k 个概率
        # ================================================================
        """
        【代码详解】：
        cov[k] = float(p[order[:k]].sum())

        【逐步拆解】：
        1. order[:k]：取前 k 个索引
           例如：order[:3] = [4, 0, 2]

        2. p[order[:k]]：取对应的概率
           例如：p[[4, 0, 2]] = [0.3, 0.3, 0.25]

        3. .sum()：求和
           例如：0.3 + 0.3 + 0.25 = 0.85

        【给零基础同学的解释】：
        想象你要计算"前3名学生的总分"：
        - 第1名：95分
        - 第2名：90分
        - 第3名：85分
        - 总分：95 + 90 + 85 = 270分
        """
        cov[k] = float(p[order[:k]].sum())

    return cov


# ============================================================================
# 第2部分完成
# ============================================================================
# 下一部分将包含：
# - build_token_vectors 函数的详细注释
# - Hard/Soft 口径的详细解释
# - 置信度特征的计算方法
# ============================================================================
